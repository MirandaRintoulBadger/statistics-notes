<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Estimation | Statistics 371 Full Notes</title>
  <meta name="description" content="Introductory Applied Statistics for the Life Sciences" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Estimation | Statistics 371 Full Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Introductory Applied Statistics for the Life Sciences" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Estimation | Statistics 371 Full Notes" />
  
  <meta name="twitter:description" content="Introductory Applied Statistics for the Life Sciences" />
  

<meta name="author" content="Miranda Rintoul" />


<meta name="date" content="2024-09-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-variables.html"/>
<link rel="next" href="confidence-intervals.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics 371 Full Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction To Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#statistics"><i class="fa fa-check"></i><b>1.1</b> Statistics</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-terms"><i class="fa fa-check"></i><b>1.2</b> Key terms</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#types-of-data"><i class="fa fa-check"></i><b>1.3</b> Types of data</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#course-outline"><i class="fa fa-check"></i><b>1.4</b> Course outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#histograms"><i class="fa fa-check"></i><b>2.1</b> Histograms</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#location"><i class="fa fa-check"></i><b>2.2</b> Location</a></li>
<li class="chapter" data-level="2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#spread"><i class="fa fa-check"></i><b>2.3</b> Spread</a></li>
<li class="chapter" data-level="2.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#box-plots"><i class="fa fa-check"></i><b>2.4</b> Box plots</a></li>
<li class="chapter" data-level="2.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#multiple-datasets"><i class="fa fa-check"></i><b>2.5</b> Multiple datasets</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sampling"><i class="fa fa-check"></i><b>3.1</b> Sampling</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#probability-basics"><i class="fa fa-check"></i><b>3.2</b> Probability basics</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>3.4</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#random-variable-basics"><i class="fa fa-check"></i><b>4.1</b> Random variable basics</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#expectation-and-variance"><i class="fa fa-check"></i><b>4.2</b> Expectation and variance</a></li>
<li class="chapter" data-level="4.3" data-path="random-variables.html"><a href="random-variables.html#binomial-random-variables"><i class="fa fa-check"></i><b>4.3</b> Binomial random variables</a></li>
<li class="chapter" data-level="4.4" data-path="random-variables.html"><a href="random-variables.html#rules-of-expectation-and-variance"><i class="fa fa-check"></i><b>4.4</b> Rules of expectation and variance</a></li>
<li class="chapter" data-level="4.5" data-path="random-variables.html"><a href="random-variables.html#normal-random-variables"><i class="fa fa-check"></i><b>4.5</b> Normal random variables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-1"><i class="fa fa-check"></i><b>5.1</b> Estimation</a></li>
<li class="chapter" data-level="5.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions"><i class="fa fa-check"></i><b>5.2</b> Sampling distributions</a></li>
<li class="chapter" data-level="5.3" data-path="estimation.html"><a href="estimation.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="6.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#z-confidence-interval"><i class="fa fa-check"></i><b>6.1</b> Z confidence interval</a></li>
<li class="chapter" data-level="6.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-interval-interpretation"><i class="fa fa-check"></i><b>6.2</b> Confidence interval interpretation</a></li>
<li class="chapter" data-level="6.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-confidence-interval"><i class="fa fa-check"></i><b>6.3</b> T confidence interval</a></li>
<li class="chapter" data-level="6.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#proportion-ci"><i class="fa fa-check"></i><b>6.4</b> Proportion CI</a></li>
<li class="chapter" data-level="6.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#bootstrap-confidence-interval"><i class="fa fa-check"></i><b>6.5</b> Bootstrap confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>7.1</b> One-sample T test</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#errors"><i class="fa fa-check"></i><b>7.2</b> Errors</a></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sided-tests"><i class="fa fa-check"></i><b>7.3</b> One-sided tests</a></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-z-test"><i class="fa fa-check"></i><b>7.4</b> One-sample Z test</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#power"><i class="fa fa-check"></i><b>7.5</b> Power</a></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#bootstrap-test"><i class="fa fa-check"></i><b>7.6</b> Bootstrap test</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html"><i class="fa fa-check"></i><b>8</b> Other One-Sample Tests</a>
<ul>
<li class="chapter" data-level="8.1" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html#one-sample-proportion-test"><i class="fa fa-check"></i><b>8.1</b> One-sample proportion test</a></li>
<li class="chapter" data-level="8.2" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html#median-test"><i class="fa fa-check"></i><b>8.2</b> Median test</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="two-sample-testing.html"><a href="two-sample-testing.html"><i class="fa fa-check"></i><b>9</b> Two-Sample Testing</a>
<ul>
<li class="chapter" data-level="9.1" data-path="two-sample-testing.html"><a href="two-sample-testing.html#equal-variances-t-test"><i class="fa fa-check"></i><b>9.1</b> Equal variances T test</a></li>
<li class="chapter" data-level="9.2" data-path="two-sample-testing.html"><a href="two-sample-testing.html#unequal-variances-t-test"><i class="fa fa-check"></i><b>9.2</b> Unequal variances T test</a></li>
<li class="chapter" data-level="9.3" data-path="two-sample-testing.html"><a href="two-sample-testing.html#two-sample-proportion-test"><i class="fa fa-check"></i><b>9.3</b> Two-sample proportion test</a></li>
<li class="chapter" data-level="9.4" data-path="two-sample-testing.html"><a href="two-sample-testing.html#two-sample-bootstrap-test"><i class="fa fa-check"></i><b>9.4</b> Two-sample bootstrap test</a></li>
<li class="chapter" data-level="9.5" data-path="two-sample-testing.html"><a href="two-sample-testing.html#rank-sum-test"><i class="fa fa-check"></i><b>9.5</b> Rank sum test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="testing-paired-data.html"><a href="testing-paired-data.html"><i class="fa fa-check"></i><b>10</b> Testing Paired Data</a>
<ul>
<li class="chapter" data-level="10.1" data-path="testing-paired-data.html"><a href="testing-paired-data.html#paired-t-test"><i class="fa fa-check"></i><b>10.1</b> Paired T test</a></li>
<li class="chapter" data-level="10.2" data-path="testing-paired-data.html"><a href="testing-paired-data.html#signed-rank-test"><i class="fa fa-check"></i><b>10.2</b> Signed rank test</a></li>
<li class="chapter" data-level="10.3" data-path="testing-paired-data.html"><a href="testing-paired-data.html#median-test-1"><i class="fa fa-check"></i><b>10.3</b> Median test</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>11</b> Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#more-than-two-groups"><i class="fa fa-check"></i><b>11.1</b> More than two groups</a></li>
<li class="chapter" data-level="11.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#variance-decomposition"><i class="fa fa-check"></i><b>11.2</b> Variance decomposition</a></li>
<li class="chapter" data-level="11.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#anova-test"><i class="fa fa-check"></i><b>11.3</b> ANOVA test</a></li>
<li class="chapter" data-level="11.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#post-hoc-analysis"><i class="fa fa-check"></i><b>11.4</b> Post-hoc analysis</a></li>
<li class="chapter" data-level="11.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#non-normal-data"><i class="fa fa-check"></i><b>11.5</b> Non-normal data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>12</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="linear-regression.html"><a href="linear-regression.html#correlation"><i class="fa fa-check"></i><b>12.1</b> Correlation</a></li>
<li class="chapter" data-level="12.2" data-path="linear-regression.html"><a href="linear-regression.html#linear-modeling"><i class="fa fa-check"></i><b>12.2</b> Linear modeling</a></li>
<li class="chapter" data-level="12.3" data-path="linear-regression.html"><a href="linear-regression.html#testing-slope"><i class="fa fa-check"></i><b>12.3</b> Testing slope</a></li>
<li class="chapter" data-level="12.4" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i><b>12.4</b> Prediction</a></li>
<li class="chapter" data-level="12.5" data-path="linear-regression.html"><a href="linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>12.5</b> Coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>13</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#goodness-of-fit-test"><i class="fa fa-check"></i><b>13.1</b> Goodness-of-fit test</a></li>
<li class="chapter" data-level="13.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#independence-test"><i class="fa fa-check"></i><b>13.2</b> Independence test</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics 371 Full Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Estimation<a href="estimation.html#estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="figs/comics/ch5.png" height="400px" style="display: block; margin: auto;" /></p>
<p>We study populations of interest by taking a random sample. But because the sample is random, the sample quantities (such as sample mean or sample sd) are also random. Studying the random behavior of these statistics helps us develop formal methods to analyze the corresponding parameters.</p>
<div id="estimation-1" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Estimation<a href="estimation.html#estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A grocery store is planning to buy 50 commercially raised chickens (broilers). The seller claims the weights of their broilers are approximately normal with mean 1.387 kg and standard deviation 0.161 kg. So, the claim is that the weight of each broiler is being sampled from <span class="math inline">\(W \sim N(1.387, 0.161^2)\)</span>.</p>
<p>The weight of the 50 specific broilers received in the shipment had an observed mean weight <span class="math inline">\(\bar{w} = 1.2\)</span> kg with standard deviation 0.173 kg. The manager expected a mean broiler weight of 1.387. Should he complain to the supplier, or did he just get unlucky? What other information would we like to know?</p>
<p>Let random variable <span class="math inline">\(W\)</span> represent the weights of broilers. If we plan to take a sample of 50 weights, then each item in our sample is a random variable equal to <span class="math inline">\(W\)</span>.
<span class="math display">\[W_1, W_2, W_3, \ldots , W_{50}\]</span>
These variables are random, because the weights have some random variability. The <span class="math inline">\(W_i\)</span> will realize to 50 constant values <em>after</em> we measure the 50 broilers we happened to get.</p>
<p>We assume that the <span class="math inline">\(W_i\)</span> are independent of each other and are all drawn from the exact same population.</p>
<div class="infobox deff">
<p>In general, when we have a sample of data, we assume that the observations are <strong>indepenent and identically distributed</strong> (iid) random variables. That is to say,</p>
<ul>
<li>The are independent.</li>
<li>They have the same distribution (given by the population).</li>
</ul>
</div>
<hr />
<p>We are thinking about data in more abstract terms, rather than as a collection of numbers. By treating the observations as random variables, we can use them to represent <em>all possible samples</em> of size <span class="math inline">\(n\)</span> that we might end up getting. It’s not a set of particular numbers, it’s a procedure for how we’re going to get our numbers.</p>
<p>When we calculate a statistic like the mean from a random sample, it is a function of random variables, and therefore is a RV itself. The sample mean weight is
<span class="math display">\[\bar{W} = \frac{1}{n}\sum_{i=1}^n W_i\]</span>
We’re using a capital <span class="math inline">\(\bar{W}\)</span> here to indicate that it is random. The value of the mean depends on what we observe in our random sample. We might get a bunch of lightweight broilers, or we might get lucky and observe a bunch of heavy broilers.</p>
<p>Intuitively, if the true mean weight is claimed to be 1.387, then the mean of our observed sample, <span class="math inline">\(\bar{W}\)</span>, feels like it ought to be close to 1.387.</p>
<hr />
<p>In general, we use a sample mean <span class="math inline">\(\bar{X}\)</span> (a statistic) to estimate a population mean <span class="math inline">\(\mu\)</span> (a parameter). <span class="math inline">\(\bar{X}\)</span> is an estimator of the population mean <span class="math inline">\(\mu\)</span>.</p>
<div class="infobox deff">
<p>An <strong>estimator</strong> is a statistic that comes from a random sample and serves as a “guess” for a parameter’s value. An estimator is random.</p>
</div>
<p>A lot of our discussion will focus on using <span class="math inline">\(\bar{X}\)</span> as an estimator for <span class="math inline">\(\mu\)</span>, but there are other types of estimation for other parameters.</p>
<p>It’s useful to figure out how we can study <span class="math inline">\(\mu\)</span> in the case of the broilers. We might suspect that the true mean weight is not actually equal to 1.387. So we can use our observed value of <span class="math inline">\(\bar{W}\)</span> to learn about <span class="math inline">\(\mu\)</span>.</p>
<hr />
<p>Let’s briefly clarify the vocabulary. The formula that describes how we use sample data to approximate a parameter is an estimat<strong>OR</strong>, e.g. the sample mean. For the broilers,
<span class="math display">\[\text{Estimator: }\;\; \frac{1}{50}\sum_{i=1}^{50} W_i \;=\; \bar{W}.\]</span>
This is a procedure, or recipe, for how we would estimate the mean if we had a sample of size 50.</p>
<p>The numerical value computed from the actual observed data is an estimat<strong>E</strong>.
<span class="math display">\[\text{Estimate: }\;\; \frac{w_1 + w_2 + \cdots w_{50}}{50} \;=\; 1.2 \;=\; \bar{w}.\]</span>
This is an actual number, not a formula. Notice that we use the lowercase <span class="math inline">\(\bar{w}\)</span> to indicate a measured value of the sample mean. An estimator is a RV, and an estimate is a realization of that RV.</p>
<hr />
<p>The seller claims each <span class="math inline">\(W_i \sim N(1.387, 0.161^2)\)</span>. We expect the observed sample mean to be about 1.387, but we got <span class="math inline">\(\bar{w} = 1.2\)</span>. Is this a problem, or is it just up to chance?</p>
<p>In other words, is the value <span class="math inline">\(\bar{w} = 1.2\)</span> consistent with <span class="math inline">\(\mu = 1.387\)</span>? To answer this quesstion, we need to know what a “typical” value of <span class="math inline">\(\bar{W}\)</span> looks like. This is possible because <span class="math inline">\(\bar{W}\)</span> is a random variable. Is 1.2 something that we are likely to see, or is it really out of the ordinary?</p>
<p>We consider all of the possible values of <span class="math inline">\(\bar{W}\)</span> and their corresponding probabilities.</p>
</div>
<div id="sampling-distributions" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Sampling distributions<a href="estimation.html#sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since <span class="math inline">\(\bar{W}\)</span> is random, it has its own probability function that describes its behavior and distribution. We might ask, “what is the probability of observing a sample mean of 1.2 or smaller?”</p>
<div class="infobox deff">
<p>The probability function of an estimator is called the <strong>sampling distribution</strong>.</p>
</div>
<p>This works the same way as other probability functions, but it has a special name because it applies to an estimator. The sampling distribution of <span class="math inline">\(\bar{W}\)</span> describes all of the possible mean weights we can observe, depending on our sample.</p>
<hr />
<p>Let’s look at a much smaller example, so we can have a concrete illustration of how sampling distributions work. Consider a discrete population where 20% of the population has value 0, 40% has value 2, and 40% has value 4.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center">0</th>
<th align="center">2</th>
<th align="center">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = x)\)</span></td>
<td align="center">0.2</td>
<td align="center">0.4</td>
<td align="center">0.4</td>
</tr>
</tbody>
</table>
<p>Imagine taking a sample of size 2 <span class="math inline">\((X_1, X_2)\)</span> from this population. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and identically distributed (iid) RVs that can take on the value 0, 2, or 4. So, these are all of the possible samples we might see:</p>
<table>
<colgroup>
<col width="22%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X_1, X_2\)</span></td>
<td>0, 0</td>
<td>0, 2</td>
<td>0, 4</td>
<td>2, 0</td>
<td>2, 2</td>
<td>2, 4</td>
<td>4, 0</td>
<td>4, 2</td>
<td>4, 4</td>
</tr>
</tbody>
</table>
<p>Consider the following statistics we could calculate from our sample of 2:</p>
<ul>
<li>Sample mean
<span class="math display">\[\bar{X} = \frac{X_1 + X_2}{2}\]</span></li>
<li>Sample standard deviation
<span class="math display">\[S = \sqrt{\frac{1}{1}\sum(X-\bar{X})^2}\]</span></li>
<li>Sample maximum
<span class="math display">\[\text{whichever of }X_1, X_2 \text{ is larger}\]</span></li>
<li>Sample total
<span class="math display">\[T = X_1 + X_2\]</span></li>
</ul>
<p>The 4 statistics above are all RVs that depend on the value of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. For example, if the sample was (0, 2), the sample mean would be 1. But if the sample was (2, 2), the sample mean would be 2.</p>
<p>With a fairly small example, we can exhaustively calculate the value of each statistic for every possible sample.</p>
<table>
<colgroup>
<col width="22%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(X_1, X_2\)</span></th>
<th align="center">0, 0</th>
<th align="center">0, 2</th>
<th align="center">0, 4</th>
<th align="center">2, 0</th>
<th align="center">2, 2</th>
<th align="center">2, 4</th>
<th align="center">4, 0</th>
<th align="center">4, 2</th>
<th align="center">4, 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean <span class="math inline">\(\bar{x}\)</span></td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td>Total <span class="math inline">\(t\)</span></td>
<td align="center">0</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">6</td>
<td align="center">4</td>
<td align="center">6</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td>Max</td>
<td align="center">0</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td>SD <span class="math inline">\(s\)</span></td>
<td align="center">0</td>
<td align="center">1.41</td>
<td align="center">2.83</td>
<td align="center">1.41</td>
<td align="center">0</td>
<td align="center">1.41</td>
<td align="center">2.83</td>
<td align="center">1.41</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Each row in the table gives the sampling distribution of the statistic, for all possible values of (<span class="math inline">\(X_1, X_2\)</span>).</p>
<div class="infobox exer">
<p>The four histograms below each depict a different sampling distribution from the table above. Match the four histograms to the 4 statistics (sample mean, sample total, sample max, and sample sd). Note that the x axes are not the same for all of the graphs.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p><span style="color:#8601AF">
Distribution A describes the sample maximum. According to the table, the maximum can be either 0, 2, or 4. It’s very rarely 0, since this only happens if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are both 0. Occasionally it has value 2, but most of the time, the sample maximum is 4.
</span></p>
<p><span style="color:#8601AF">
Distribution B describes the sample mean. Its possible values are 0, 1, 2, 3, or 4, which match the values in the “mean” row of the table. We see that the mean is more likely to be a middle value of 2 or 3, and less likely to be an extreme value like 0 or 4.
</span></p>
<p><span style="color:#8601AF">
Distribution C describes the sample sd. The sample sd can be 1.41 or 2.83 depending on the values in the sample. But if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> end up being equal, then the sd is 0.
</span></p>
<p><span style="color:#8601AF">
Distribution D describes the sample total. Its possible values are 0, 2, 4, 6, or 8. We see that it has a similar shape to the sample mean, where the middle values are more likely.
</span></p>
</div>
<p>The value of each estimator depends on the items that appear in the sample. The same principle applies to more complicated or larger examples, such as the broiler weights.</p>
<hr />
<p>Let’s re-focus on the sample mean <span class="math inline">\(\bar{X}\)</span> (or <span class="math inline">\(\bar{W}\)</span>). We know that we can use it to estimate the population mean <span class="math inline">\(\mu\)</span>, but why do we want to do this? Does this measure do a “good” job of estimating <span class="math inline">\(\mu\)</span>?</p>
<p>We need to look at the expectation and variance properties of <span class="math inline">\(\bar{X}\)</span> to better understand how it works as an estimator.</p>
<p>The expectation tells us the average value of the sample mean (across all samples). The expected value of the sample mean is <span class="math inline">\(E(\bar{X}) = \mu\)</span>. Its expected value is exactly the parameter that we are trying to estimate. On average, the sample mean is located at the true mean.</p>
<p>Some samples might have an unexpectedly large or unexpectedly small sample mean, just due to random variance. But the value of the sample mean averages out to the true center of the population.</p>
<p>The variance determines the variability in the sample mean from sample to sample. We can think of <span class="math inline">\(V(\bar{X})\)</span> as “estimation error”. The variance is <span class="math inline">\(V(\bar{X}) = \frac{\sigma^2}{n}\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the original population variance. We see that as we take a bigger sample, the estimation error goes down.</p>
<div class="infobox deff">
<p>In summary, <span class="math inline">\(E(\bar{X}) = \mu, \quad V(\bar{X}) = \frac{\sigma^2}{n}\)</span>.</p>
</div>
<hr />
<p>Here are the formal derivations that show the mean and variance of <span class="math inline">\(\bar{X}\)</span>. Note that for the variance calculation, we are using the fact that the indidivudal <span class="math inline">\(X_i\)</span> are independent. <span class="math display">\[\begin{align}
E(\bar{X}) &amp;= E\Big(\frac{1}{n}\sum_{i=1}^n X_i\Big) \\
&amp;= \frac{1}{n}E\Big(\sum_{i=1}^n X_i\Big) \\
&amp;= \frac{1}{n}\sum_{i=1}^n E(X_i) \\
&amp;= \frac{1}{n}\cdot n\mu \\
&amp;= \mu
\\ \\
V(\bar{X}) &amp;= V\Big(\frac{1}{n}\sum_{i=1}^n X_i\Big) \\
&amp;= \frac{1}{n^2}V\Big(\sum_{i=1}^n X_i\Big) \\
&amp;= \frac{1}{n^2}\sum_{i=1}^n V(X_i) \quad \quad \text{by independence}\\
&amp;= \frac{1}{n^2}\cdot n\sigma^2 \\
&amp;= \frac{\sigma^2}{n}
\end{align}\]</span></p>
<hr />
<div class="infobox deff">
<p>The standard deviation of an estimator is called <strong>standard error</strong>. This is to differentiate it from the standard deviation of the original population.</p>
</div>
<p>The standard error of <span class="math inline">\(\bar{X}\)</span> is <span class="math inline">\(\sqrt{V(\bar{X})} = \frac{\sigma}{\sqrt{n}}\)</span>. This corresponds to how much uncertainty we have when we use <span class="math inline">\(\bar{X}\)</span> as a way to estimate the value of <span class="math inline">\(\mu\)</span>. Lower standard error is better!</p>
<hr />
<p>So why is <span class="math inline">\(\bar{X}\)</span> considered a good estimator of <span class="math inline">\(\mu\)</span>. We saw that <span class="math inline">\(E(\bar{X}) = \mu\)</span>, which means the sample mean is located at the true mean on average. We say that <span class="math inline">\(\bar{X}\)</span> is unbiased for <span class="math inline">\(\mu\)</span>.</p>
<div class="infobox deff">
<p>In general, an <strong>unbiased</strong> estimator has expected value equal to the desired parameter. It is neither an underestimate or overestimate. If the expected value is less than the parameter, it is <strong>biased downward</strong> and is an underestimate. If the expected value is greater than the parameter, it is <strong>biased upward</strong> and is an overestimate.</p>
</div>
<p>Another nice property of <span class="math inline">\(\bar{X}\)</span> is that its standard error (and variance) goes down as the sample size <span class="math inline">\(n\)</span> increases. So by taking a bigger sample, we know that we are doing a better job of estimating <span class="math inline">\(\mu\)</span>. In general, bigger samples are desirable for estimation.</p>
<hr />
<p>We like to have estimators that have both low bias and low variance. We can imagine the true parameter as the bullseye of a target and each shot being a “guess” at the parameter using our estimator.</p>
<p><img src="figs/est/bias-variance.png" /><!-- --></p>
<p>Unfortunately, there is an inherent tradeoff between the two. With a finite sample of data, we cannot have arbitrarily low bias AND arbitrarily low variance at the same time.</p>
<hr />
<p>Using bias and variance, we can also compare <em>different</em> estimation techniques for the <em>same</em> parameter. The sample mean <span class="math inline">\(\bar{X}\)</span> has a unique quality of having the smallest variance of all unbiased estimators of <span class="math inline">\(\mu\)</span>. Let’s illustrate with a sample of size 2. If <span class="math inline">\(n = 2\)</span>,
<span class="math display">\[\bar{X} = \frac{X_1 + X_2}{2}, \quad E(\bar{X}) = \mu, \quad V(\bar{X}) = \frac{\sigma^2}{2}.\]</span>
Let’s define a new estimator for <span class="math inline">\(\mu\)</span> called <span class="math inline">\(\tilde{X}\)</span> that is an unfairly weighted average of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.
<span class="math display">\[\tilde{X} = \frac{X_1 + 2X_2}{3}\]</span>
We can show that <span class="math inline">\(E(\tilde{X}) = \mu\)</span>, which means that both <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\tilde{X}\)</span> are unbiased estimators of the true mean. However, <span class="math inline">\(V(\tilde{X}) = \frac{5\sigma^2}{9}\)</span>, which is slightly larger than <span class="math inline">\(V(\bar{X})\)</span>. Because both estimators are unbiased, but <span class="math inline">\(\bar{X}\)</span> has lower variance, it is preferred.</p>
<div class="infobox pond">
<p>Try to come up with an estimator for <span class="math inline">\(\mu\)</span> that has more bias but less variance than <span class="math inline">\(\bar{X}\)</span>.</p>
</div>
<hr />
<p>When trying to analyze the population mean, it is useful to know the variance of <span class="math inline">\(\bar{X}\)</span>, which is <span class="math inline">\(\frac{\sigma^2}{n}\)</span>. But <span class="math inline">\(\sigma^2\)</span>, the population variance, is also a parameter. It is very rarely known. So, we have to estimate the population spread with our data.</p>
<p>We have seen formulas for calculating the sample variance and sample sd of a set of data. These are estimators for the variance and sd of the population. Again, because these quantities are based on a random sample, we treat them as random variables. The sample variance is
<span class="math display">\[S^2 \;=\; \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2\]</span>
We divide by <span class="math inline">\(n-1\)</span> because it makes <span class="math inline">\(S^2\)</span> unbiased for <span class="math inline">\(\sigma^2\)</span>. Dividing by <span class="math inline">\(n\)</span> underestimates the variance. The sample standard deviation is just the square root of the sample variance.
<span class="math display">\[S \;=\; \sqrt{\frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2}\]</span>
Interestingly, although <span class="math inline">\(S^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(S\)</span> is a slightly biased estimator of <span class="math inline">\(\sigma\)</span>. However, this bias is minimal and we generally don’t worry about it.</p>
<p>So, we have formulas to estimate the population mean, variance, and sd from some observed data.</p>
<table>
<thead>
<tr class="header">
<th>Definition</th>
<th>Statistic</th>
<th></th>
<th>Parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean</td>
<td><span class="math inline">\(\bar{X}\)</span></td>
<td>estimates</td>
<td><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="even">
<td>Variance</td>
<td><span class="math inline">\(S^2\)</span></td>
<td>estimates</td>
<td><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="odd">
<td>St. dev.</td>
<td><span class="math inline">\(S\)</span></td>
<td>estimates</td>
<td><span class="math inline">\(\sigma\)</span></td>
</tr>
</tbody>
</table>
<hr />
<p>Now that we can estimate the population variance/sd, we can estimate the variance of <span class="math inline">\(\bar{X}\)</span>. Substituting <span class="math inline">\(S\)</span> for <span class="math inline">\(\sigma\)</span>, the estimated variance and standard error of <span class="math inline">\(\bar{X}\)</span> is
<span class="math display">\[\hat{V}(\bar{X}) \;=\; \frac{S^2}{n}, \quad \quad \hat{se}(\bar{X}) \;=\; \frac{S}{\sqrt{n}}\]</span>
We can get both <span class="math inline">\(S\)</span> and <span class="math inline">\(n\)</span> from our data.</p>
<hr />
<p>In our original example, we observed a mean broiler weight <span class="math inline">\(\bar{w} = 1.2\)</span> even though the distributor claims the population mean weight is <span class="math inline">\(\mu = 1.387\)</span>. We want to figure out if we are likely to observe a mean weight of 1.2 or less, using the distribution of the random variable <span class="math inline">\(\bar{W}\)</span>.</p>
<p>Let’s start with the nicest case, where the original population is normal. Imagine we take a sample from a normal population, and calculate the sample mean.</p>
<p>We’ve learned about the center and spread of <span class="math inline">\(\bar{X}\)</span> (or <span class="math inline">\(\bar{W}\)</span>). But to answer our question, we need to know the specific shape of the sampling distribution. Intuituively, if we get a good spread of different values, the sample mean would end up close to the true mean <span class="math inline">\(\mu\)</span>.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-4-1.png" width="672" height="120%" /></p>
<p>If we happen to get several small values in our sample, the sample mean would be a bit to the left of <span class="math inline">\(\mu\)</span>.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-5-1.png" width="672" height="120%" /></p>
<p>If we happen to get several large values in our sample, the sample mean would be a bit to the right of <span class="math inline">\(\mu\)</span>.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-6-1.png" width="672" height="120%" /></p>
<p>If the underlying population is normal, then the sampling distribution of <span class="math inline">\(\bar{X}\)</span> also has a normal bell-curve shape, with the same center and less spread.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-7-1.png" width="672" height="120%" /></p>
<p>The sampling distribution (dashed line) represents all possible values of the sample mean we might observe when taking a sample from the population (solid line).</p>
<hr />
<div class="infobox deff">
<p>Formally, the items in our sample are iid normal:
<span class="math display">\[X_i \sim N(\mu, \sigma^2)\]</span>
Then the distribution of the sample mean <span class="math inline">\(\bar{X}\)</span> is also normal.<br />
<span class="math display">\[\bar{X} \sim N\Big(\mu, \frac{\sigma^2}{n}\Big)\]</span></p>
</div>
<p>We learned the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span> of <span class="math inline">\(\bar{X}\)</span> before, and now we know the shape. This is true because of a useful property about normal RVs.</p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> be independent normal RVs, and <span class="math inline">\(c_1, c_2, \ldots, c_n\)</span> be any constants. Then if we take each normal RV, multiply it by a constant, and add them up, then the RV
<span class="math display">\[c_1 X_1 + c_2 X_2 + \cdots + c_n X_n\]</span>
is also normal. The sample mean is one such example of this pattern, where each <span class="math inline">\(X_i\)</span> is a draw from a normal population and each constant <span class="math inline">\(c_i = \frac{1}{n}\)</span>.</p>
<p><span class="math display">\[\bar{X} \quad = \quad \frac{1}{n}\sum_{i=1}^n X_i \quad =\quad \frac{1}{n}X_1 + \frac{1}{n}X_2 + \cdots + \frac{1}{n}X_n\]</span></p>
<hr />
<p>In the broiler example, we assume the weights of broilers have a normal distribution with mean 1.387 kg and standard deviation 0.161 kg. <span class="math inline">\(W \sim N(1.387, 0.161^2)\)</span>.</p>
<div class="infobox exer">
<ul>
<li>If you randomly select 1 broiler from this distribution, what is the probability that its weight is below 1.2 kg?</li>
</ul>
<p><span style="color:#8601AF">
We need to calculate a normal probability on the original population distribution, which is something we have done before in chapter 4. We need <span class="math inline">\(P(W \le 1.2)\)</span>, which is about 0.123.
</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="estimation.html#cb1-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.2</span>, <span class="fl">1.387</span>, <span class="fl">0.161</span>)</span></code></pre></div>
<pre><code>## [1] 0.1227212</code></pre>
<p><span style="color:#8601AF">
If we’re just drawing one item from the population, it’s not super unlikely for it to be less than 1.2 kg.
</span></p>
<ul>
<li>What is the sampling distribution for <span class="math inline">\(\bar{W}_5\)</span>, the average weight of 5 broilers randomly sampled from this distribution?</li>
</ul>
<p><span style="color:#8601AF">
We have seen formulas for the expectation and variance of the sample mean. <span class="math inline">\(E(\bar{W}_5) = \mu  = 1.387\)</span>, and <span class="math inline">\(V(\bar{W}_5) = \frac{\sigma^2}{n} = \frac{0.161^2}{5}\)</span>. Furthermore, we know that since the original population is normal, the sampling distribution of <span class="math inline">\(\bar{W}\)</span> must also be normal. So, <span class="math inline">\(\bar{W}\)</span> is normal with mean 1.387 and variance <span class="math inline">\(\frac{0.161^2}{5}.\)</span>
<span class="math display">\[\bar{W}_5 \sim N\Big(1.387, \frac{0.161^2}{5}\Big)\]</span>
</span></p>
<ul>
<li>If you randomly select 5 broilers, what is the probability that the mean weight of the 5 is below 1.2 kg?</li>
</ul>
<p><span style="color:#8601AF">
Now, we need to calculate a probability on the sampling distribution that we identified in the previous part. We can also do this with <code>pnorm</code>, but remember that R wants the standard deviation rather than the variance. The standard deviation of <span class="math inline">\(\bar{W}_5\)</span> (called standard error) is <span class="math inline">\(\sqrt{\frac{0.161^2}{5}} = \frac{0.161}{\sqrt{5}}\)</span> According to R, <span class="math inline">\(P(\bar{W}_5 \le 1.2) = 0.005\)</span>.
</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="estimation.html#cb3-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.2</span>, <span class="fl">1.387</span>, <span class="fl">0.161</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">5</span>))</span></code></pre></div>
<pre><code>## [1] 0.00469974</code></pre>
<p><span style="color:#8601AF">
This is a very small probability (a 0.5% chance). When we take a sample of size 5, we expect the sample values to be spread out across the population, so it would be unlikely that the mean of all of the values would be less than 1.2.
</span></p>
<ul>
<li>If you randomly select 50 broilers, what is the probability that the mean weight of the 50 is below 1.2 kg?</li>
</ul>
<p><span style="color:#8601AF">
Now, we need to find the sampling distribution of <span class="math inline">\(\bar{W_{50}}\)</span>. The expected value is <span class="math inline">\(E(\bar{W}_50) = \mu = 1.387\)</span> and the variance is <span class="math inline">\(V(\bar{W}_{50}) = \frac{\sigma^2}{n} = \frac{0.161^2}{50}\)</span>. Just like <span class="math inline">\(\bar{W}_5\)</span>, <span class="math inline">\(\bar{W}_{50}\)</span> is a normal RV. So,
<span class="math display">\[\bar{W}_{50} \sim N\Big(1.387, \frac{0.161^2}{50}\Big).\]</span>
According to R, the <span class="math inline">\(P(\bar{W}_{50} \le 1.2) = 1.08\times 10^{-16}\)</span>.
</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="estimation.html#cb5-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.2</span>, <span class="fl">1.387</span>, <span class="fl">0.161</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">50</span>))</span></code></pre></div>
<pre><code>## [1] 1.078833e-16</code></pre>
<p><span style="color:#8601AF">
That’s a decimal point followed by 15 zeroes! So, the probability is essentially 0. If the true mean were 1.387, it would be incredibly likely to observe a mean weight of 1.2 or lower for 50 broilers. So, this might lead us to beleive that the claim of <span class="math inline">\(\mu = 1.387\)</span> is false.
</span></p>
</div>
<p>Here’s an illustration of the broiler population and each of the sampling distributions, with the value 1.2 marked.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<hr />
<p>By now, we have studied the sample mean <span class="math inline">\(\bar{X}\)</span> at length. Another useful statistic is the sample total (or sum), <span class="math inline">\(T\)</span>. This is just the sum of all of our observations.
<span class="math display">\[T \;=\; X_1 + X_2 + \cdots X_3 \;=\; \sum_{i=1}^n X_i\]</span>
Because it is calculated from random quantities, <span class="math inline">\(T\)</span> is a random variable with its own mean, variance, and sampling distribution.</p>
<p>If the mean of a population is <span class="math inline">\(\mu\)</span>, and we draw <span class="math inline">\(n\)</span> items, we intuitively expect the total to be about <span class="math inline">\(n\mu\)</span>. And it does turn out that the expected value of the total is <span class="math inline">\(E(T) = n\mu\)</span>. The variance of the total also scales with <span class="math inline">\(n\)</span>: <span class="math inline">\(V(T) = n\sigma^2\)</span>. Note that, while the variability of the sample mean decreasees as <span class="math inline">\(n\)</span> gets bigger, the variability of the sample total increases with <span class="math inline">\(n\)</span>.</p>
<hr />
<p>Here are the formal derivations that show the mean and variance of <span class="math inline">\(T\)</span>. Note that for the variance calculation, we are using the fact that the indidivudal <span class="math inline">\(X_i\)</span> are independent. <span class="math display">\[\begin{align}
E(T) &amp;= E\Big(\sum_{i=1}^n X_i\Big) \\
&amp;= \sum_{i=1}^n E(X_i) \\
&amp;= n\mu
\\ \\
V(T) &amp;= V\Big(\sum_{i=1}^n X_i\Big) \\
&amp;= \sum_{i=1}^n V(X_i) \quad \quad \text{by independence}\\
&amp;= n\sigma^2
\end{align}\]</span></p>
<hr />
<p>What about the shape of the distribution of <span class="math inline">\(T\)</span>? As it turns out, if we have a normal population, then the sampling distribution of <span class="math inline">\(T\)</span> is also normal. Remember our rule where we can add independent normal RVs and get another normal RV:
<span class="math display">\[c_1 X_1 + c_2 X_2 + \cdots + c_n X_n\]</span>
The total <span class="math inline">\(T\)</span> is just an example of this rule where each <span class="math inline">\(c_i = 1\)</span>.</p>
<div class="infobox deff">
<p>Formally, the items in our sample are iid normal:
<span class="math display">\[X_i \sim N(\mu, \sigma^2)\]</span>
Then the distribution of the sample total <span class="math inline">\(T\)</span> is also normal.<br />
<span class="math display">\[T \sim N(n\mu, n\sigma^2)\]</span></p>
</div>
<p>The sample total is just the sample mean multiplied by <span class="math inline">\(n\)</span>, so it is sensible that the two statistics should have similar behavior.</p>
<p>In the broiler example, we assume the weights of broilers have a normal distribution with mean 1.387 kg and standard deviation 0.161 kg. <span class="math inline">\(W \sim N(1.387, 0.161^2)\)</span>.</p>
<div class="infobox exer">
<ul>
<li>What is the sampling distribution for <span class="math inline">\(T_{50}\)</span>, the total weight of 50 randomly selected broilers from this distribution?</li>
</ul>
<p><span style="color:#8601AF">
We know that the average weight of 50 broilers is <span class="math inline">\(E(T_{50}) = n\mu = 50(1.387)\)</span>. The variance of this total is given by <span class="math inline">\(V(T_{50}) = n\sigma^2 = 50(0.161^2)\)</span>. Finally, because the underlying population <span class="math inline">\(W\)</span> is normal, the sample total must also be normal. We get
<span class="math display">\[T_{50} \sim N\Big(50(1.387), 50(0.161^2)\Big)\]</span>
</span></p>
<ul>
<li>The distributor is trying to estimate shipping costs. What is the 95th percentile for the total weight of 50 broilers? That is, below what weight will 95% of draws from <span class="math inline">\(T_{50}\)</span> fall?</li>
</ul>
<p><span style="color:#8601AF">
This question is asking us to find the 95th percentile of the sampling distribution of <span class="math inline">\(T_{50}\)</span>. This is the value such that 95% of the realizations of <span class="math inline">\(T_{50}\)</span> are less than this value. We can find this with R’s <code>qnorm</code>, either directly or by transforming the 95th percentile of a standard normal. Note that we have to use the square root of the variance, <span class="math inline">\(\sqrt{50(0.161^2)} = \sqrt{50}(0.161)\)</span>.
</span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="estimation.html#cb7-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="dv">50</span><span class="sc">*</span>(<span class="fl">1.387</span>), <span class="fu">sqrt</span>(<span class="dv">50</span>)<span class="sc">*</span><span class="fl">0.161</span>)</span></code></pre></div>
<pre><code>## [1] 71.22257</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="estimation.html#cb9-1" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb9-2"><a href="estimation.html#cb9-2" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.95</span>)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">50</span>)<span class="sc">*</span><span class="fl">0.161</span> <span class="sc">+</span> <span class="dv">50</span><span class="sc">*</span>(<span class="fl">1.387</span>)</span></code></pre></div>
<pre><code>## [1] 71.22257</code></pre>
<p><span style="color:#8601AF">
The 95th percentile is about 71.22 kg. 95% of the time, the total weight of 50 random broilers will be less than this value.
</span></p>
</div>
</div>
<div id="central-limit-theorem" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Central Limit Theorem<a href="estimation.html#central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we assume that the underlying population is normal (the “normality assumption”), we can do a lot. This assumption tells us that the sample mean and sample sum are also both normal.</p>
<p>The normality assumption is fairly safe for many populations. There are a lot of real-life processes that result in an approximately normal shape. But, not every population is normal, and so we want some way to assess the validity of the normality assumption.</p>
<p>We have a sample of observed numeric data, and we want to guess whether the underlying population is normal.</p>
<hr />
<div class="infobox deff">
<p>A useful visual tool for verifying normality is a <strong>normal quantile-quantile plot</strong>, or qq-plot. We compare our data to a theoretical normal distribution, and plot it. If the data is normal, the points will fall on a straight diagonal line.</p>
</div>
<p>Let’s look at a qq-plot of some normal data.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>It’s not perfect, but it is certainly good enough. There’s always going to be a bit of natural variation present, even when our data is perfectly normal.</p>
<hr />
<p>We can make a qq-plot in R with the <code>qqnorm</code> function, and draw a line with <code>qqline</code>. This is the code used to make the previous plot.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="estimation.html#cb11-1" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.29</span>, <span class="fl">1.12</span>, <span class="fl">0.88</span>, <span class="fl">1.65</span>, <span class="fl">1.48</span>, <span class="fl">1.59</span>, <span class="fl">1.04</span>, <span class="fl">0.83</span>,</span>
<span id="cb11-2"><a href="estimation.html#cb11-2" tabindex="-1"></a>          <span class="fl">1.76</span>, <span class="fl">1.31</span>, <span class="fl">0.88</span>, <span class="fl">1.71</span>, <span class="fl">1.83</span>, <span class="fl">1.09</span>, <span class="fl">1.62</span>, <span class="fl">1.49</span>)</span>
<span id="cb11-3"><a href="estimation.html#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="estimation.html#cb11-4" tabindex="-1"></a><span class="fu">qqnorm</span>(data); <span class="fu">qqline</span>(data)</span></code></pre></div>
<p>R is a good way to let us explore what a “good” qq-plot looks like. The <code>rnorm</code> function lets us take a sample of normal data. Let’s make some example qq-plots of normal data.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="estimation.html#cb12-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">371</span>) <span class="co"># set RNG</span></span>
<span id="cb12-2"><a href="estimation.html#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="estimation.html#cb12-3" tabindex="-1"></a>data1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">15</span>)</span>
<span id="cb12-4"><a href="estimation.html#cb12-4" tabindex="-1"></a><span class="fu">qqnorm</span>(data1); <span class="fu">qqline</span>(data1)</span></code></pre></div>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="estimation.html#cb13-1" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">15</span>)</span>
<span id="cb13-2"><a href="estimation.html#cb13-2" tabindex="-1"></a><span class="fu">qqnorm</span>(data2); <span class="fu">qqline</span>(data2)</span></code></pre></div>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="estimation.html#cb14-1" tabindex="-1"></a>data3 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">15</span>)</span>
<span id="cb14-2"><a href="estimation.html#cb14-2" tabindex="-1"></a><span class="fu">qqnorm</span>(data3); <span class="fu">qqline</span>(data3)</span></code></pre></div>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
<p>We see that there is some natural variation present, but the points still tend to fall along the main diagonal line. Don’t be too picky about this!</p>
<hr />
<p>When data violates the normality assumption, there will generally be obvious problems in the plot. Let’s see some examples of bimodal, left-skewed, and right-skewed data in a qq-plot and corresponding histogram.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-16-1.png" width="672" /><img src="05-estimation_files/figure-html/unnamed-chunk-16-2.png" width="672" /><img src="05-estimation_files/figure-html/unnamed-chunk-16-3.png" width="672" /></p>
<p>It is probably not safe to assume these three sets of data were drawn from a normal population.</p>
<hr />
<p>Most of our statistical methods are based on this normality assumption. The results of these tests should only be accurate and meaningful if our data is normal. But in practice, data does not need to be perfectly normal for these methods to work. Most pretty good” data is “good enough”, especially if it is a large sample.</p>
<p>This is all thanks to the <strong>Central Limit Theorem</strong>, or CLT. The CLT is a powerful theorem that says that our statistical methods based on normality will still work, even if our data is not perfectly normal.</p>
<hr />
<p>When we are doing statistics on the mean <span class="math inline">\(\mu\)</span>, it doesn’t matter if our <em>data</em> is normal. What matters is that the <em>sample mean</em> <span class="math inline">\(\bar{X}\)</span> is normal. And that is exactly what the CLT gives us.</p>
<p>Intuitively, if we take a large sample from a population, their values should “cancel out” such that the sample mean almost always ends up somewhere in the middle. This produces a bell-curve shape, even when the underlying population does not have a bell curve.</p>
<hr />
<p>Formally, let <span class="math inline">\(X_1, X_2, \ldots X_n\)</span> be independent and identically distributed RVs drawn from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="infobox deff">
<p>The <strong>Central Limit Theorem</strong> says that, for <em>large enough</em> <span class="math inline">\(n\)</span>, the distribution of <span class="math inline">\(\bar{X}\)</span> is <em>approximately</em> normal with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span>.
<span class="math display">\[\bar{X} \;\dot{ \sim }\; N\Big(\mu, \frac{\sigma^2}{n}\Big)\]</span></p>
</div>
<p>We use <span class="math inline">\(\dot{\sim}\)</span> to indicate an approximate probability distribution. What’s key here is that we have not made any assumption about the underlying population shape. We’ve only specified a population mean and variance. But as long as the sample is large enough, we can safely assume the sample mean is normal.</p>
<hr />
<p>Here’s an example of a skewed population, the sampling distribution of <span class="math inline">\(\bar{X}\)</span> when <span class="math inline">\(n = 5\)</span>, and the sampling distribution of <span class="math inline">\(\bar{X}\)</span> when <span class="math inline">\(n = 20\)</span>.</p>
<p><img src="05-estimation_files/figure-html/unnamed-chunk-17-1.png" width="672" /><img src="05-estimation_files/figure-html/unnamed-chunk-17-2.png" width="672" /><img src="05-estimation_files/figure-html/unnamed-chunk-17-3.png" width="672" /></p>
<p>When <span class="math inline">\(n = 5\)</span>, the sampling distribution of <span class="math inline">\(\bar{X}\)</span> is still a little bit skewed. But for <span class="math inline">\(n = 20\)</span>, the sampling distribution resembles a bell curve.</p>
<hr />
<p>The distribution of <span class="math inline">\(\bar{X}\)</span> is more closely approximated by a normal when <span class="math inline">\(n\)</span> is larger. But how big does <span class="math inline">\(n\)</span> need to be before the normal approximation is safe to use? This depends on the shape of the underlying population (the <span class="math inline">\(X_i\)</span>).</p>
<p>When the underlying population is more normal, the CLT kicks in even when we have a small sample size. For distributions that are reasonably symmetric with no outliers, we might be able to use an <span class="math inline">\(n\)</span> as small as 5.</p>
<p>On the other hand, for extremely skewed distributions, we might need <span class="math inline">\(n\)</span> in the hundreds. We can assess the shape with a qq-plot. For most real-world data, <span class="math inline">\(n = 30\)</span> is a safe cutoff.</p>
<div class="infobox pond">
<p><a href="http://onlinestatbook.com/stat_sim/sampling_dist/index.html">This website</a> is a great tool for experimenting with the CLT for differently shaped populations.</p>
</div>
<hr />
<p>The CLT also works for the sample sum.</p>
<div class="infobox deff">
<p>For <span class="math inline">\(X_1, \ldots X_n\)</span> iid, for large enough <span class="math inline">\(n\)</span>, central limit theorem says the distribution of the sample total is approximately
<span class="math display">\[T\;\dot{ \sim }\; N(n\mu, n\sigma^2).\]</span></p>
</div>
<p>Just like with the mean, this approximation is better when <span class="math inline">\(n\)</span> is large and when the original distribution is closer to a normal.</p>
<hr />
<p><em>Lymantria dispar</em> is an invasive moth species. A state agriculture department places traps throughout the state to detect the moths. Based on years of data, the distribution of moth counts (<span class="math inline">\(C\)</span>) is discrete, with a mean of 0.5. and a standard deviation of 0.7.</p>
<p>What do we know about the distribution of <span class="math inline">\(C\)</span>? The population has a lower bound of 0, and no theoretical upper bound. This fact, along with the fact that the mean is 0.5 and the sd is 0.7, implies the distribution of <span class="math inline">\(C\)</span> is likely very right skewed. Most traps only have a small number of moths, but a few traps have a large number of moths.</p>
<div class="infobox exer">
<p>The department is concerned about unusually high numbers of moths. To test this, one state ranger takes a sample of 10 traps, and another takes a sample of 100 traps.</p>
<ul>
<li>What is the probability that the mean number of moths in 10 traps is above 0.75?</li>
<li>What is the probability that the mean number of moths in 100 traps is above 0.75?</li>
</ul>
<p>Which of these questions can we answer, and why?</p>
<p><span style="color:#8601AF">
The first question cannot be answered. We can figure out <span class="math inline">\(E(\bar{C}_{10})\)</span> and <span class="math inline">\(V(\bar{C}_{10})\)</span>, where <span class="math inline">\(\bar{C}_{10}\)</span> is the mean number of moths in 10 traps. However, we cannot answer any probability questions because we do not know the shape of the sampling distribution of <span class="math inline">\(\bar{C}_{10}\)</span>. We cannot safely assume it is normal, since the underlying population is very skewed and <span class="math inline">\(n = 10\)</span> is probably not big enough to use the CLT.
</span></p>
<p><span style="color:#8601AF">
The second question has a much bigger sample size of 100. Even though the original population is skewed, the CLT says that it is probably safe to assume that the mean number of moths in 100 traps <span class="math inline">\(\bar{C}_{100}\)</span> is approximately normal. Its expected value is <span class="math inline">\(E(\bar{C}_{100}) = \mu = 0.5\)</span> and its variance is <span class="math inline">\(V(\bar{C}_{100}) = \frac{\sigma^2}{n} = \frac{0.7^2}{100}\)</span>. So,
<span class="math display">\[\bar{C}_{100}\;\dot{ \sim }\; N\Big(0.5, \frac{0.7^2}{100}\Big).\]</span>
We need to find the area on this normal curve above 0.75, which can be done with R.
</span></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="estimation.html#cb15-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">100</span>), <span class="at">lower.tail =</span> F)</span></code></pre></div>
<pre><code>## [1] 0.0001775197</code></pre>
<p><span style="color:#8601AF">
The probability is about 0.0002, which is very small. If we observed this result, that would make me worry that the population mean moth count is not actually 0.5.
</span></p>
</div>
<hr />
<p>Let’s discuss another type of statistic, the sample proportion. This is similar to the sample mean, but we think about it in a slightly different way.</p>
<p>Our context is that we’re working with a binary population. For example, hospital reports that 7% of blood recipients contract some form of viral hepatitis. We can think of this population as being comprised of 93% zeroes (the patients who don’t have hepatitis) and 7% ones (the patients who do). Let’s call it <span class="math inline">\(Y\)</span>.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbb{P}(Y = y)\)</span></td>
<td>0.93</td>
<td>0.07</td>
</tr>
</tbody>
</table>
<p>We can assign the values 0 and 1 however we want, but we usually let the value 1 correspond to an item that meets some kind of criteria. In this context, the parameter of interest is the population proportion <span class="math inline">\(\pi = 0.07\)</span>. Note that the population mean is <span class="math inline">\(E(Y) = 0.07\)</span>, same as <span class="math inline">\(\pi\)</span>. The variance is <span class="math inline">\(V(Y) = \pi(1-\pi)\)</span>.</p>
<div class="infobox deff">
<p>The population proportion <span class="math inline">\(\pi\)</span> corresponds to the proportion of items that have value 1 in a 0-1 binary population. This is the same as the probability that a randomly chosen individual will have value 1.</p>
</div>
<hr />
<p>Let’s think about how we would learn about the proportion if we didn’t know the value of <span class="math inline">\(\pi\)</span>. We want to take a sample from the population, and find the proportion of items in the <em>sample</em> that have value 1. In this example, if we take a sample of size 30, our sample proportion would be the number of patients with hepatitis divided by 30.</p>
<p>But recall that every item in our population is either 1 (for hepatitis) or 0 (for no hepatitis). The count of patients with hepatitis is the same as the number of 1’s in our dataset, which is the same as the sum of all of our observations. We add a 1 for each patient who has hepatitis, and add 0 otherwise. In this way, we count the number of observations that meet our criteria.</p>
<p><span class="math display">\[\frac{\text{count out of 30 who have viral hepatitis}}{30} \;=\; \frac{1}{30}\sum_{i=1}^{30} y_i\]</span></p>
<div class="infobox deff">
<p>In general, the sample proportion <span class="math inline">\(\hat{p}\)</span> estimates a population proportion <span class="math inline">\(\pi\)</span>.
<span class="math display">\[\hat{p} \;=\; \frac{1}{n}\sum_{i=1}^n Y_i\]</span></p>
</div>
<p>Notice that the general formula for the proportion looks exactly like the formula for calculating the sample mean. In fact, the proportion is a specific case of the sample mean, where we are taking the mean of a sample from a 0-1 binary population.</p>
<p>Additionally, the population proportion is the same as the population mean of a 0-1 binary population.</p>
<hr />
<p>A sample proportion is a type of estimator, and it is useful to consider the expected value, variance, and sampling distribution of estimators.</p>
<div class="infobox deff">
<p>The expected value of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\mathbb{E}(\hat{p}) = \pi\)</span>. The variance of <span class="math inline">\(\hat{p}\)</span> is given by
<span class="math display">\[\mathbb{V}(\hat{p}) = \frac{\pi(1-\pi)}{n}.\]</span></p>
</div>
<p>We see that <span class="math inline">\(\hat{p}\)</span> is an unbiased estimator for the true proportion <span class="math inline">\(\pi\)</span>. We also see that the estimation error goes down as <span class="math inline">\(n\)</span> increases.</p>
<p>Here are the formal derivations that show the mean and variance of <span class="math inline">\(\hat{p}\)</span>. Note that for the variance calculation, we are using the fact that the indidivudal <span class="math inline">\(X_i\)</span> are independent. <span class="math display">\[\begin{align}
E(\hat{p}) &amp;= E\Big(\frac{1}{n}\sum_{i=1}^n Y_i\Big) \\
&amp;= \frac{1}{n}E\Big(\sum_{i=1}^n Y_i\Big) \\
&amp;= \frac{1}{n}\sum_{i=1}^n E(Y_i) \\
&amp;= \frac{1}{n}\cdot n\pi \\
&amp;= \pi
\\ \\
V(\hat{p}) &amp;= V\Big(\frac{1}{n}\sum_{i=1}^n Y_i\Big) \\
&amp;= \frac{1}{n^2}V\Big(\sum_{i=1}^n Y_i\Big) \\
&amp;= \frac{1}{n^2}\sum_{i=1}^n V(Y_i) \quad \quad \text{by independence}\\
&amp;= \frac{1}{n^2}\cdot n\pi(1-\pi) \\
&amp;= \frac{\pi(1-\pi)}{n}
\end{align}\]</span></p>
<hr />
<p>What about the shape of the sampling distribution? The exact sampling distribution is difficult to work with. However, since the sample proportion is a special case of the sample mean, the CLT applies to <span class="math inline">\(\hat{p}\)</span> as well. For a large enough sample, we can use a normal approximation for <span class="math inline">\(\hat{p}\)</span>.</p>
<div class="infobox deff">
<p>The CLT says that, for large enough <span class="math inline">\(n\)</span>, the distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal with mean <span class="math inline">\(\pi\)</span> and variance <span class="math inline">\(\frac{\pi(1-\pi)}{n}\)</span>.
<span class="math display">\[\hat{p} \;\dot{ \sim }\; N\Big(\pi, \frac{\pi(1-\pi)}{n}\Big)\]</span></p>
</div>
<p>We use a slightly different criteria for checking wither <span class="math inline">\(n\)</span> is big enough to use the CLT. The normal approximation works better when <span class="math inline">\(\hat{p}\)</span> is close to 0.5, and it is worse when <span class="math inline">\(\hat{p}\)</span> is close to 0 or 1. So the threshold depends on both <span class="math inline">\(\hat{p}\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>Typically, we want to have <span class="math inline">\(n\hat{p} \ge 5\)</span> and <span class="math inline">\(n(1-\hat{p}) \ge 5\)</span> in order to use the CLT. That is to say, we want to have at least five “successes” and “failures” in our sample.</p>
<hr />
<p>Let’s go back to the earlier example, where the proportion of blood recipients who contract hepatitis is stated to be <span class="math inline">\(\pi = 0.07\)</span>. We take a sample of size 300 and find that 8 out of 300 patients contracted hepatitis.</p>
<div class="infobox exer">
<ul>
<li>What is the sampling distribution of <span class="math inline">\(\hat{p}\)</span>?</li>
</ul>
<p><span style="color:#8601AF">
We know that the mean of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(E(\hat{p}) = \pi = 0.07\)</span>. Furthermore, the variance is <span class="math inline">\(V(\hat{p}) = \frac{\pi(1-\pi)}{n} = \frac{0.07(0.93)}{300}\)</span>. Additionally, we have 8 “successes” and 292 “failures” in our sample, which are both larger than 5. So, we can use the CLT to approximate <span class="math inline">\(\hat{p}\)</span> as a normal RV.
<span class="math display">\[\hat{p} \;\dot{ \sim }\; N\Big(0.07, \frac{0.07(0.93)}{300}\Big)\]</span>
</span></p>
<ul>
<li>Using the distribution above, find
<span class="math display">\[\mathbb{P}\Big(\hat{p} \le \frac{8}{300}\Big) = \mathbb{P}(\hat{p} \le 0.0267)\]</span></li>
</ul>
<p><span style="color:#8601AF">
We need to find a normal probability on the distribution we just identified, which we can do with <code>pnorm</code>.
</span></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="estimation.html#cb17-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">0.0267</span>, <span class="fl">0.07</span>, <span class="fu">sqrt</span>((<span class="fl">0.07</span><span class="sc">*</span><span class="fl">0.93</span>)<span class="sc">/</span><span class="dv">300</span>))</span></code></pre></div>
<pre><code>## [1] 0.001644266</code></pre>
<p><span style="color:#8601AF">
The probability is quite small at 0.0016. If the true hepatitis proportion is <span class="math inline">\(\pi = 0.07\)</span>, then it is unlikely that only 8 or fewer patients out of 300 would have hepatitis.
</span></p>
</div>
<hr />
<p>For a binary 0-1 popultion, the sample proportion is analogous to the sample mean. The sample <strong>count</strong> is analogous to the sample total. This is just counting the number of items in our sample that meet the criteria, without dividing by <span class="math inline">\(n\)</span>. If <span class="math inline">\(Y\)</span> is a binary population,</p>
<p><span class="math display">\[\text{Sample count } \;=\; \sum_{i=1}^n Y_i\]</span></p>
<div class="infobox pond">
<p>Suppose we take a sample of size <span class="math inline">\(n\)</span> from a binary population with true proportion <span class="math inline">\(\pi\)</span>. What is the expected value and variance of the sample count?</p>
</div>
<hr />
<p>At this point, we’ve developed the mathematical foundations we need to do statistics. In the next section, we’ll start discussing actual statistical methods.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="confidence-intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
