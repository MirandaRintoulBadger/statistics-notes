<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Two-Sample Testing | Statistics 371 Full Notes</title>
  <meta name="description" content="Introductory Applied Statistics for the Life Sciences" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Two-Sample Testing | Statistics 371 Full Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Introductory Applied Statistics for the Life Sciences" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Two-Sample Testing | Statistics 371 Full Notes" />
  
  <meta name="twitter:description" content="Introductory Applied Statistics for the Life Sciences" />
  

<meta name="author" content="Miranda Rintoul" />


<meta name="date" content="2024-08-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="other-one-sample-tests.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics 371 Full Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction To Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#statistics"><i class="fa fa-check"></i><b>1.1</b> Statistics</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#key-terms"><i class="fa fa-check"></i><b>1.2</b> Key terms</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#types-of-data"><i class="fa fa-check"></i><b>1.3</b> Types of data</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#course-outline"><i class="fa fa-check"></i><b>1.4</b> Course outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#histograms"><i class="fa fa-check"></i><b>2.1</b> Histograms</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#location"><i class="fa fa-check"></i><b>2.2</b> Location</a></li>
<li class="chapter" data-level="2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#spread"><i class="fa fa-check"></i><b>2.3</b> Spread</a></li>
<li class="chapter" data-level="2.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#box-plots"><i class="fa fa-check"></i><b>2.4</b> Box plots</a></li>
<li class="chapter" data-level="2.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#multiple-datasets"><i class="fa fa-check"></i><b>2.5</b> Multiple datasets</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sampling"><i class="fa fa-check"></i><b>3.1</b> Sampling</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#probability-basics"><i class="fa fa-check"></i><b>3.2</b> Probability basics</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>3.4</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#random-variable-basics"><i class="fa fa-check"></i><b>4.1</b> Random variable basics</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#expectation-and-variance"><i class="fa fa-check"></i><b>4.2</b> Expectation and variance</a></li>
<li class="chapter" data-level="4.3" data-path="random-variables.html"><a href="random-variables.html#binomial-random-variables"><i class="fa fa-check"></i><b>4.3</b> Binomial random variables</a></li>
<li class="chapter" data-level="4.4" data-path="random-variables.html"><a href="random-variables.html#rules-of-expectation-and-variance"><i class="fa fa-check"></i><b>4.4</b> Rules of expectation and variance</a></li>
<li class="chapter" data-level="4.5" data-path="random-variables.html"><a href="random-variables.html#normal-random-variables"><i class="fa fa-check"></i><b>4.5</b> Normal random variables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-1"><i class="fa fa-check"></i><b>5.1</b> Estimation</a></li>
<li class="chapter" data-level="5.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions"><i class="fa fa-check"></i><b>5.2</b> Sampling distributions</a></li>
<li class="chapter" data-level="5.3" data-path="estimation.html"><a href="estimation.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="6.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#z-confidence-interval"><i class="fa fa-check"></i><b>6.1</b> Z confidence interval</a></li>
<li class="chapter" data-level="6.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-interval-interpretation"><i class="fa fa-check"></i><b>6.2</b> Confidence interval interpretation</a></li>
<li class="chapter" data-level="6.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#t-confidence-interval"><i class="fa fa-check"></i><b>6.3</b> T confidence interval</a></li>
<li class="chapter" data-level="6.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#proportion-ci"><i class="fa fa-check"></i><b>6.4</b> Proportion CI</a></li>
<li class="chapter" data-level="6.5" data-path="confidence-intervals.html"><a href="confidence-intervals.html#bootstrap-confidence-interval"><i class="fa fa-check"></i><b>6.5</b> Bootstrap confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-t-test"><i class="fa fa-check"></i><b>7.1</b> One-sample T test</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#errors"><i class="fa fa-check"></i><b>7.2</b> Errors</a></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sided-tests"><i class="fa fa-check"></i><b>7.3</b> One-sided tests</a></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#one-sample-z-test"><i class="fa fa-check"></i><b>7.4</b> One-sample Z test</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#power"><i class="fa fa-check"></i><b>7.5</b> Power</a></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#bootstrap-test"><i class="fa fa-check"></i><b>7.6</b> Bootstrap test</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html"><i class="fa fa-check"></i><b>8</b> Other ONe-Sample Tests</a>
<ul>
<li class="chapter" data-level="8.1" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html#one-sample-proportion-test"><i class="fa fa-check"></i><b>8.1</b> One-sample proportion test</a></li>
<li class="chapter" data-level="8.2" data-path="other-one-sample-tests.html"><a href="other-one-sample-tests.html#median-test"><i class="fa fa-check"></i><b>8.2</b> Median test</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="two-sample-testing.html"><a href="two-sample-testing.html"><i class="fa fa-check"></i><b>9</b> Two-Sample Testing</a>
<ul>
<li class="chapter" data-level="9.1" data-path="two-sample-testing.html"><a href="two-sample-testing.html#equal-variances-t-test"><i class="fa fa-check"></i><b>9.1</b> Equal variances T test</a></li>
<li class="chapter" data-level="9.2" data-path="two-sample-testing.html"><a href="two-sample-testing.html#unequal-variances-t-test"><i class="fa fa-check"></i><b>9.2</b> Unequal variances T test</a></li>
<li class="chapter" data-level="9.3" data-path="two-sample-testing.html"><a href="two-sample-testing.html#two-sample-proportion-test"><i class="fa fa-check"></i><b>9.3</b> Two-sample proportion test</a></li>
<li class="chapter" data-level="9.4" data-path="two-sample-testing.html"><a href="two-sample-testing.html#two-sample-bootstrap-test"><i class="fa fa-check"></i><b>9.4</b> Two-sample bootstrap test</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics 371 Full Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="two-sample-testing" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Two-Sample Testing<a href="two-sample-testing.html#two-sample-testing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="figs/comics/ch9.png" width="300px" style="display: block; margin: auto;" /></p>
<p>We have learned several techniques for comparing a parameter of interest to a fixed value, such as comparing the mean <span class="math inline">\(\mu\)</span> to <span class="math inline">\(\mu_0 = 1.5\)</span>. These tests all apply to a single sample.</p>
<p>It is also common to have <em>two</em> populations of interest that we want to compare them to each other. In this chapter, we’ll cover different techniques for analyzing a difference in two populations.</p>
<div id="equal-variances-t-test" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Equal variances T test<a href="two-sample-testing.html#equal-variances-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The T test from chapter 7 is a population technique for analyzing a single population mean <span class="math inline">\(\mu\)</span>. Now, we will generalize the concepts of a basic T hypothesis test to the problem of comparing two means. Generally, we are interested in looking at the difference in two population means, <span class="math inline">\(\mu_1 - \mu_2\)</span>.</p>
<p>he horned lizard has a frill of spikes around its head which are thought to provide protection from predators. Researchers want to compare the spikes of lizards killed by predators to live lizards from the same area. The live and dead lizards make up two separate populations.</p>
<p>Samples were taken from each group, and the longest spike on each lizard was measured in mm. Our research question is, “Is there any difference in the mean length of the longest spike between the dead and live lizard populations?”</p>
<hr />
<p>The lizard data is given below. Let’s perform a quick exploratory analysis.</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{Dead: }&amp;17.65, 20.83, 24.59, 18.52, 21.40, 23.78, \\ &amp; 20.36, 18.83, 21.83, 20.06 \\ \\
\text{Alive: }&amp;23.76, 21.17, 26.13, 20.18, 23.01, 24.84, \\ &amp; 19.34, 24.94, 27.14, 25.87, 18.95, 22.61
\end{eqnarray*}\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="two-sample-testing.html#cb1-1" tabindex="-1"></a>dead <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">17.65</span>, <span class="fl">20.83</span>, <span class="fl">24.59</span>, <span class="fl">18.52</span>, <span class="fl">21.40</span>, <span class="fl">23.78</span>,</span>
<span id="cb1-2"><a href="two-sample-testing.html#cb1-2" tabindex="-1"></a>          <span class="fl">20.36</span>, <span class="fl">18.83</span>, <span class="fl">21.83</span>, <span class="fl">20.06</span>)</span>
<span id="cb1-3"><a href="two-sample-testing.html#cb1-3" tabindex="-1"></a>live <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">23.76</span>, <span class="fl">21.17</span>, <span class="fl">26.13</span>, <span class="fl">20.18</span>, <span class="fl">23.01</span>, <span class="fl">24.84</span>,</span>
<span id="cb1-4"><a href="two-sample-testing.html#cb1-4" tabindex="-1"></a>           <span class="fl">19.34</span>, <span class="fl">24.94</span>, <span class="fl">27.14</span>, <span class="fl">25.87</span>, <span class="fl">18.95</span>, <span class="fl">22.61</span>)</span></code></pre></div>
<p>Let’s calculate the numeric summary measures for each group.</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(n\)</span></th>
<th align="center"><span class="math inline">\(\bar{x}\)</span></th>
<th align="center"><span class="math inline">\(s\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Dead</td>
<td align="center">10</td>
<td align="center">20.78</td>
<td align="center">2.22</td>
</tr>
<tr class="even">
<td align="center">Live</td>
<td align="center">12</td>
<td align="center">23.16</td>
<td align="center">2.76</td>
</tr>
</tbody>
</table>
<p>The mean of the live group is larger, which suggests they tend to have larger spikes than the dead lizards. We also see that both groups have a fairly similar standard deviation.</p>
<p>We can also make some visual summaries of the groups. Comparative histograms and boxplots are useful as are qq-plots of each group.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="two-sample-testing.html#cb2-1" tabindex="-1"></a>bin_breaks <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">22</span>, <span class="dv">24</span>, <span class="dv">26</span>, <span class="dv">28</span>)</span>
<span id="cb2-2"><a href="two-sample-testing.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="two-sample-testing.html#cb2-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># View 2 plots at once</span></span>
<span id="cb2-4"><a href="two-sample-testing.html#cb2-4" tabindex="-1"></a><span class="co"># Set xlim, ylim, and breaks to be the same</span></span>
<span id="cb2-5"><a href="two-sample-testing.html#cb2-5" tabindex="-1"></a><span class="fu">hist</span>(dead, <span class="at">main =</span> <span class="st">&quot;Dead&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>),</span>
<span id="cb2-6"><a href="two-sample-testing.html#cb2-6" tabindex="-1"></a>     <span class="at">breaks =</span> bin_breaks, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span>
<span id="cb2-7"><a href="two-sample-testing.html#cb2-7" tabindex="-1"></a><span class="fu">hist</span>(live, <span class="at">main =</span> <span class="st">&quot;Live&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>),</span>
<span id="cb2-8"><a href="two-sample-testing.html#cb2-8" tabindex="-1"></a>     <span class="at">breaks =</span> bin_breaks, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="two-sample-testing.html#cb3-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-2"><a href="two-sample-testing.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="two-sample-testing.html#cb3-3" tabindex="-1"></a><span class="fu">boxplot</span>(dead, live, <span class="at">ylab =</span> <span class="st">&quot;Longest Spike Length (mm)&quot;</span>,</span>
<span id="cb3-4"><a href="two-sample-testing.html#cb3-4" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">names =</span> <span class="fu">c</span>(<span class="st">&quot;Dead&quot;</span>, <span class="st">&quot;Live&quot;</span>))</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<p>The plots visually confirm the slight shift in center between the two groups.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="two-sample-testing.html#cb4-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># View 2 plots at once</span></span>
<span id="cb4-2"><a href="two-sample-testing.html#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="two-sample-testing.html#cb4-3" tabindex="-1"></a><span class="fu">qqnorm</span>(dead, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Dead&quot;</span>)</span>
<span id="cb4-4"><a href="two-sample-testing.html#cb4-4" tabindex="-1"></a><span class="fu">qqnorm</span>(live, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Live&quot;</span>)</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="two-sample-testing.html#cb5-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p>Additionally, the two groups appear to be normal.</p>
<hr />
<p>Now, let’s write hypotheses. We want to know if the live and dead lizard groups have the same spike length. The null (uninteresting) result is that the two groups are the same. This gives hypotheses</p>
<p><span class="math display">\[H_0: \mu_{dead} = \mu_{live}\quad\quad\text{versus}\quad\quad H_A: \mu_{dead} \neq \mu_{live}.\]</span></p>
<p>Conventionally, we write the hypotheses in terms of a difference in parameters, so that both parameters are on the same side of the equals sign. An equivalent pair of hypotheses is</p>
<p><span class="math display">\[H_0: \mu_{dead} - \mu_{live} = 0\quad\quad\text{versus}\quad\quad H_A: \mu_{dead} - \mu_{live} \neq 0.\]</span></p>
<p>So we are testing the difference <span class="math inline">\(\mu_{dead} - \mu_{live}\)</span> againat a null value of 0. A null hypothesis won’t always have the value 0, but it is common.</p>
<p>The test statistic will be based on the difference in the observed means, <span class="math inline">\(\bar{X}_{dead} - \bar{X}_{live}\)</span>. If the null hypothesis is true, then the difference will be close to 0.</p>
<hr />
<p>To formalize the test statistic, let’s actually review the test statistic we used for the one-sample T test. For testing <span class="math inline">\(\mu = \mu_0\)</span>, we use
<span class="math display">\[T \;=\; \frac{\bar{X} - \mu_0}{S/\sqrt{n}}.\]</span></p>
<p>Our two-sample T test statistic will have the same structure, but the specific formula will be different since we are dealing with two groups instead of 1.</p>
<div class="infobox deff">
<p>The general formula for a T test statistic is
<span class="math display">\[T \;=\; \frac{\text{estimate from data} - \text{value under }H_0}{\text{estimated standard error of estimate}}\]</span></p>
</div>
<p>In the one-sample case, the estimate is <span class="math inline">\(\bar{X}\)</span>, the value under <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\mu_0\)</span>, and the standard error is <span class="math inline">\(S/\sqrt{n}\)</span>.</p>
<p>For the lizard example, the estimated difference from the data is <span class="math inline">\(\bar{X}_{dead} - \bar{X}_{live}\)</span>. The value of the difference under the null is 0. So, we need to find a formula for the estimated standard error of <span class="math inline">\(\bar{X}_{dead} - \bar{X}_{live}\)</span> to be the denominator of our test statistic.</p>
<hr />
<p>To estimate the standard error, we are going to make a specific assumption about our data. Suppose the two groups have equal population variance. That is to say <span class="math inline">\(\sigma^2_{dead}\)</span> and <span class="math inline">\(\sigma^2_{live}\)</span> are both equal to some common variance <span class="math inline">\(\sigma^2\)</span>.
<span class="math display">\[\sigma^2_{dead} \;=\; \sigma^2_{live} \;=\; \sigma^2.\]</span>
This tells us that
<span class="math display">\[\mathbb{V}(\bar{X}_{dead}) = \frac{\sigma^2}{n_{dead}}, \quad \quad \mathbb{V}(\bar{X}_{live}) = \frac{\sigma^2}{n_{live}}.\]</span>
When we assume the groups have a common variance, the variance of the difference is given by
<span class="math display">\[\mathbb{V}(\bar{X}_{dead} - \bar{X}_{live}) \;=\; \frac{\sigma^2}{n_{dead}} + \frac{\sigma^2}{n_{live}} \;=\; \sigma^2\Big(\frac{1}{n_{dead}} + \frac{1}{n_{live}}\Big).\]</span></p>
<div class="infobox pond">
<p>Why are the two variance terms being added instead of subtracted?</p>
</div>
<p>The standard error is the square root of this quantity.</p>
<p><span class="math display">\[se(\bar{X}_{dead} - \bar{X}_{live}) \;=\; \sigma\sqrt{\frac{1}{n_{dead}} + \frac{1}{n_{live}}}\]</span></p>
<p>The value <span class="math inline">\(\sigma\)</span> is the common standard deviation for both groups. This is a parameter, and we need to estimate it from our data in order to estimate the standard error. The esimated version uses <span class="math inline">\(s_p\)</span>, the pooled standard deviation..</p>
<p><span class="math display">\[\hat{se}(\bar{X}_{dead} - \bar{X}_{live}) \;=\; s_p \sqrt{\frac{1}{n_{dead}} + \frac{1}{n_{live}}}.\]</span></p>
<p>The pooled standard deviation is computed by combining measures of spread from both groups. First, we can get the pooled variance, which is a weighted average of the individual group variances.</p>
<div class="infobox deff">
<p>The <strong>pooled variance</strong> of two groups of data is given by
<span class="math display">\[s_p^2 \;=\; \frac{(n_{1}-1)s_{1}^2 \;+\; (n_{2}-1)s_{2}^2}{n_{1} + n_{2} - 2}.\]</span>
The <strong>pooled standard deviation</strong> is
<span class="math display">\[s_p \;=\; \sqrt{s_p^2} \;=\; \sqrt{\frac{(n_{1}-1)s_{1}^2 \;+\; (n_{2}-1)s_{2}^2}{n_{1} + n_{2} - 2}}.\]</span></p>
</div>
<p>The weights we use are based on the number of observations in each group: <span class="math inline">\(n_1 - 1\)</span> for the first group, and <span class="math inline">\(n_2 - 1\)</span> for the second group. So if one group has more observations, it will be more “important” for the purpose of calculating <span class="math inline">\(s^2_p\)</span> and <span class="math inline">\(s_p\)</span>.</p>
<div class="infobox exer">
<p>Assuming the live and dead lizard groups have the same population variance, find an estimate for the pooled variance and pooled standard deviation from the lizard data. Here are the summary statistics for the lizard data:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(n\)</span></th>
<th align="center"><span class="math inline">\(\bar{x}\)</span></th>
<th align="center"><span class="math inline">\(s\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Dead</td>
<td align="center">10</td>
<td align="center">20.78</td>
<td align="center">2.22</td>
</tr>
<tr class="even">
<td align="center">Live</td>
<td align="center">12</td>
<td align="center">23.16</td>
<td align="center">2.76</td>
</tr>
</tbody>
</table>
<p><span style="color:#8601AF">
The pooled variance is found by taking a weighted average of the two individual sample variances, where the weights are the number of observations minus 1.
<span class="math display">\[s_p^2 \;=\; \frac{(10-1)2.22^2 + (12-1)2.76^2}{10 + 12 - 2} \;=\; 6.407.\]</span>
The pooled standard deviation is the square root of the pooled variance.
<span class="math display">\[s_p \;=\; \sqrt{6.407} \;=\; 2.531.\]</span>
</span></p>
</div>
<hr />
<p>The formula for the estimated standard error goes in the denominator of our test statistic. We have to take into account the uncertainty of <span class="math inline">\(\bar{X}_1 - \bar{X}_2\)</span> when testing <span class="math inline">\(\mu_1 - \mu_2\)</span>. Our two-sample T test statistic is
<span class="math display">\[T \;=\; \frac{\bar{X}_{dead} - \bar{X}_{alive} \;-\; 0}{s_p\sqrt{\frac{1}{n_{dead}} + \frac{1}{n_{alive}}}}.\]</span>
If <span class="math inline">\(H_0\)</span> is true and <span class="math inline">\(\mu_{live} - \mu_{dead} = 0\)</span>, then the test statistic <span class="math inline">\(T\)</span> will have a t distribution with <span class="math inline">\(n_{dead} - n_{live} - 2\)</span> degrees of freedom.</p>
<p>A one-sample T test has <span class="math inline">\(n - 1\)</span> degrees of freedom. So the df for a two-sample test is <span class="math inline">\((n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2\)</span>.</p>
<p>In the lizard example, we have 10 dead observations and 12 live observations, so we have 20 degrees of freedom. To complete our test, we calculate an observed test statistic and check whether it is consistent with the <span class="math inline">\(t_{20}\)</span> curve.</p>
<hr />
<p>Before we complete our test, we should check that the necessary test assumptions are met. The two-sample T test has three assumptions: independence, normality, and constant variance.</p>
<p>The independence assumption says that each of our observations are independent. In the two-sample case, we additionally need to assume that the two groups are independent and do not affect each other. We do not know exactly, but we can probably assume this to be true for the lizard data.</p>
<p>For normality, we need to make sure that each of the two individual groups are normal. Based on the qq-plots we made before, this assumption seems to be well met for the lizard data.</p>
<p>The constant variance assumption is why we were able to calculate a pooled standard deviation <span class="math inline">\(s_p\)</span>. For this assumption to be reasonable, the two groups need to be similarly spread out. We can check this visually with a histogram or boxplot. We can also check this numerically. Typically, for the constant variance assumption to be reasonable, we want the sample SDs to be within a factor of 2. We need to have</p>
<p><span class="math display">\[0.5 \;&lt;\; \frac{s_1}{s_2} \;&lt;\; 2.\]</span>
Equivalently, we can check if the variances are within a factor of 4.</p>
<p>For the lizard data, the ratio of standard deviations is <span class="math inline">\(\frac{2.22}{2.76} = 0.804\)</span>, which is in our acceptable range. So, the three assumptions for an equal variances T test are met for this data.</p>
<div class="infobox exer">
<p>Complete the two-sample T test on the lizard data of the hypotheses
<span class="math display">\[H_0: \mu_{dead} = \mu_{live}\quad\quad\text{versus}\quad\quad H_A: \mu_{dead} \neq \mu_{live}.\]</span></p>
<ul>
<li>Use <span class="math inline">\(s_p = 2.531\)</span> to calcualte <span class="math inline">\(t_{obs}\)</span>.</li>
</ul>
<p><span style="color:#8601AF">
The observed test statistic is
<span class="math display">\[\frac{\bar{X}_{dead} - \bar{X}_{alive} \;-\; 0}{s_p\sqrt{\frac{1}{n_{dead}} + \frac{1}{n_{alive}}}} \;=\; \frac{20.78 - 23.16 \;-\; 0}{2.531\sqrt{\frac{1}{10} + \frac{1}{12}}} \;=\; -2.196.\]</span>
This value lies in the lower tail of our <span class="math inline">\(t_{20}\)</span> null distribution.
</span></p>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<ul>
<li>Find a p-value on the <span class="math inline">\(t_{20}\)</span> distribution and draw a conclusion at the 5% level.</li>
</ul>
<p><span style="color:#8601AF">
Since we are doing a two-sided test, the p-value is <span class="math inline">\(2\times\)</span> the area outside of our test statistic. Our test statistic is negative, and thus in the lower tail, so we compute a lower-tail area on the t with 20 degrees of freedom.
</span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="two-sample-testing.html#cb6-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="sc">-</span><span class="fl">2.196</span>, <span class="at">df =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## [1] 0.04005361</code></pre>
<p><span style="color:#8601AF">
We have a p-value of 0.04. This is not a super small p-value, but it is less than <span class="math inline">\(\alpha = 0.05\)</span>, so we do have sufficient evidence to reject <span class="math inline">\(H_0\)</span> at the 5% level. We conclude that the mean longest spike length is different for the dead and live lizards.
</span></p>
</div>
<hr />
<p>The one-sample T test has a corresponding one-smaple T confidence interval. Many types of statistical tests have a corresponding CI, and the same is true of the two-sample T test.</p>
<p>CIs can be built quickly if we remember the general formula for a confidence interval:
<span class="math display">\[\text{point estimate } \pm \text{ critical value }\times \text{ standard error}.\]</span>
In the two-sample case, the point estimate is <span class="math inline">\(\bar{X}_1 - \bar{X}_2\)</span>. The standard error is <span class="math inline">\(s_p(\frac{1}{n_1} + \frac{1}{n_2})\)</span>. And the critical value is the <span class="math inline">\(\alpha/2\)</span> value from the t with <span class="math inline">\(n_1 + n_2 - 2\)</span> degrees of freedom. In the lizard example, the critical value for 95% confidence is 2.086.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="two-sample-testing.html#cb8-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## [1] 2.085963</code></pre>
<p>A 95% CI for <span class="math inline">\(\mu_{dead} - \mu_{live}\)</span> in the lizards example is
<span class="math display">\[\begin{align*}
&amp; \bar{X}_{live} - \bar{X}_{dead} \;\pm\; t_{20, 0.025} \times s_p\sqrt{\frac{1}{n_{live}} + \frac{1}{n_{dead}}} \\
&amp;= 20.78 - 23.16 \;\pm \; 2.086\times 2.531\sqrt{\frac{1}{10}+\frac{1}{12}} \\
&amp;= (-4.64, -0.12)
\end{align*}\]</span></p>
<p>Notice that this interval does not contain 0, which corresponds to us rejecting <span class="math inline">\(H_0: \mu_{dead} - \nu_{live} = 0\)</span>.</p>
<hr />
<div class="infobox deff">
<p>In general, an <strong>equal variances T test</strong> can test for any difference in means:
<span class="math display">\[H_0: \mu_1 - \mu_2 = \mu_0\quad\quad\text{versus}\quad\quad H_A: \mu_1 - \mu_2 \neq \mu_0\]</span>
(or a corresponding one-sided hypothesis). The test statistic is
<span class="math display">\[T \;=\; \frac{\bar{X}_1 - \bar{X}_2 \;-\; \mu_0}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\]</span>
and the null distribution is a t distribution with <span class="math inline">\(n_1 + n_2 - 2\)</span> degrees of freedom. We can find a rejection region or calculate a p-value according to the direction of the hypotheses.</p>
</div>
<p>We can also use the <code>t.test</code> function in R to automatically perform a two-sample T test. We put both groups of data into the function, and specify the value of the difference under the null. To make the equal variance assumption, we must specify <code>var.equal= T</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="two-sample-testing.html#cb10-1" tabindex="-1"></a>dead <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">17.65</span>, <span class="fl">20.83</span>, <span class="fl">24.59</span>, <span class="fl">18.52</span>, <span class="fl">21.40</span>, <span class="fl">23.78</span>,</span>
<span id="cb10-2"><a href="two-sample-testing.html#cb10-2" tabindex="-1"></a>          <span class="fl">20.36</span>, <span class="fl">18.83</span>, <span class="fl">21.83</span>, <span class="fl">20.06</span>)</span>
<span id="cb10-3"><a href="two-sample-testing.html#cb10-3" tabindex="-1"></a>live <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">23.76</span>, <span class="fl">21.17</span>, <span class="fl">26.13</span>, <span class="fl">20.18</span>, <span class="fl">23.01</span>, <span class="fl">24.84</span>,</span>
<span id="cb10-4"><a href="two-sample-testing.html#cb10-4" tabindex="-1"></a>           <span class="fl">19.34</span>, <span class="fl">24.94</span>, <span class="fl">27.14</span>, <span class="fl">25.87</span>, <span class="fl">18.95</span>, <span class="fl">22.61</span>)</span>
<span id="cb10-5"><a href="two-sample-testing.html#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="two-sample-testing.html#cb10-6" tabindex="-1"></a><span class="fu">t.test</span>(dead, live, <span class="at">mu =</span> <span class="dv">0</span>,</span>
<span id="cb10-7"><a href="two-sample-testing.html#cb10-7" tabindex="-1"></a>       <span class="at">var.equal =</span> T,</span>
<span id="cb10-8"><a href="two-sample-testing.html#cb10-8" tabindex="-1"></a>       <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  dead and live
## t = -2.192, df = 20, p-value = 0.04038
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.6383563 -0.1149771
## sample estimates:
## mean of x mean of y 
##  20.78500  23.16167</code></pre>
</div>
<div id="unequal-variances-t-test" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Unequal variances T test<a href="two-sample-testing.html#unequal-variances-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The equal variances T test has some fairly strict assumptions, so we might want to know how to relax some of these assumptions. What if the data is normal, but the two group variances seem to be different? We can perform a different type of two-sample T test.</p>
<p>Let’s look at a new example. Concrete beams are often reinforced with another material. Eight concrete beams with fiberglass reinforcement and eleven concrete beams with carbon reinforcement were poured, and the breaking strength of each beam was measured.</p>
<p>We want to know whether the two reinforcement materials are equally strong, or whether the strength differs between the two types of beams. So, we would set up one-sided hypotheses for a difference in means, similar to the ones we had in the lizard example.</p>
<p><span class="math display">\[H_0: \mu_{fiber} - \mu_{carbon} = 0\quad\quad\text{versus}\quad\quad H_A: \mu_{fiber} - \mu_{carbon} \neq 0\]</span></p>
<p>The data and summaries are as follows (a larger number means greater strength).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="two-sample-testing.html#cb12-1" tabindex="-1"></a>fiber <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">38.3</span>, <span class="fl">29.6</span>, <span class="fl">33.4</span>, <span class="fl">33.6</span>, <span class="fl">30.7</span>, <span class="fl">32.7</span>, <span class="fl">34.6</span>, <span class="fl">32.3</span>)</span>
<span id="cb12-2"><a href="two-sample-testing.html#cb12-2" tabindex="-1"></a>carbon <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">48.8</span>, <span class="fl">38.0</span>, <span class="fl">42.2</span>, <span class="fl">45.1</span>, <span class="fl">32.8</span>, <span class="fl">47.2</span>, <span class="fl">50.6</span>,</span>
<span id="cb12-3"><a href="two-sample-testing.html#cb12-3" tabindex="-1"></a>            <span class="fl">44.0</span>, <span class="fl">43.9</span>, <span class="fl">40.4</span>, <span class="fl">45.8</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(n\)</span></th>
<th align="center"><span class="math inline">\(\bar{x}\)</span></th>
<th align="center"><span class="math inline">\(s\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Fiber</td>
<td align="center">8</td>
<td align="center">33.15</td>
<td align="center">2.63</td>
</tr>
<tr class="even">
<td align="center">Carbon</td>
<td align="center">11</td>
<td align="center">43.53</td>
<td align="center">5.06</td>
</tr>
</tbody>
</table>
<p>The mean for the carbon group looks higher, but so does the standard deviation. Let’s also explore the data visually.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="two-sample-testing.html#cb13-1" tabindex="-1"></a><span class="fu">boxplot</span>(fiber, carbon, <span class="at">ylab =</span> <span class="st">&quot;Beam Strength&quot;</span>,</span>
<span id="cb13-2"><a href="two-sample-testing.html#cb13-2" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">names =</span> <span class="fu">c</span>(<span class="st">&quot;Fiberglass&quot;</span>, <span class="st">&quot;Carbon&quot;</span>))</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="two-sample-testing.html#cb14-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># View 2 plots at once</span></span>
<span id="cb14-2"><a href="two-sample-testing.html#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="two-sample-testing.html#cb14-3" tabindex="-1"></a><span class="fu">qqnorm</span>(fiber, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Fiberglass&quot;</span>)</span>
<span id="cb14-4"><a href="two-sample-testing.html#cb14-4" tabindex="-1"></a><span class="fu">qqnorm</span>(carbon, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Carbon&quot;</span>)</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="two-sample-testing.html#cb15-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p>Let’s think about what the summaries tell us about the three assumptions for the equal variances T test. The independence and normality assumptions are probably safe. However, we might have some concerns about the equal variance assumption. Visually, the carbon group is more spread out. If we look at the ratio of sample standard deviations, we see that it is just barely within our acceptable range.
<span class="math display">\[\frac{s_{fiber}}{s_{carbon}} = \frac{2.63}{5.06} = 0.52\]</span></p>
<p>So, we might not feel comfortable assuming that the true variances of the fiberglass and carbon groups are equal. In this situation, we would perform a different type of T test, called the unequal variances T test or the Welch T test.</p>
<p>The Welch T test still requires the normality and independence assumptions, but it relaxes the equal variance assumption.</p>
<hr />
<p>The test statistic for this type of test follows the same general structure of a T test statistic:
<span class="math display">\[T \;=\; \frac{\text{estimate from data} - \text{value under }H_0}{\text{estimated standard error of estimate}}\]</span>
The estimate of <span class="math inline">\(\mu_{fiber} - \mu_{carbon}\)</span> is <span class="math inline">\(\bar{X}_{fiber} - \bar{X}_{carbon}\)</span> and the value under the null is <span class="math inline">\(\mu_0\)</span> (which is 0 for this example). But the standard error we use is different than the one for the equal variances T test.</p>
<p>The standard error for equal variances assumes the two populations share a common variance: <span class="math inline">\(\sigma^2_1 = \sigma^2_2 = \sigma\)</span>.
<span class="math display">\[se(\bar{X}_{1} - \bar{X}_{2}) \;=\; \sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}\]</span>
We estimated <span class="math inline">\(\sigma\)</span> with a pooled standard deviation based on both samples. But if we want to relax the equal variance assumption and assume <span class="math inline">\(\sigma^2_1 \neq \sigma^2_2\)</span>, we no longer have a common variance <span class="math inline">\(\sigma^2\)</span>. We need to use the more general form of the standard error of <span class="math inline">\(\bar{X}_1 - \bar{X}_2\)</span>.</p>
<hr />
<p>In the unequal variances case, we have
<span class="math display">\[\mathbb{V}(\bar{X}_{fiber}) = \frac{\sigma_{fiber}^2}{n_{fiber}}, \quad \quad \mathbb{V}(\bar{X}_{carbon}) = \frac{\sigma_{carbon}^2}{n_{carbon}}.\]</span>
The variance of the difference in sample means is
<span class="math display">\[\mathbb{V}(\bar{X}_{fiber} - \bar{X}_{carbon}) \;=\; \frac{\sigma_{fiber}^2}{n_{fiber}} + \frac{\sigma_{carbon}^2}{n_{carbon}}.\]</span>
So, the estimated standard error is given by
<span class="math display">\[\hat{se}(\bar{X}_{fiber} - \bar{X}_{carbon}) \;=\; \sqrt{\frac{s^2_{fiber}}{n_{fiber}} + \frac{s^2_{carbon}}{n_{carbon}}}.\]</span></p>
<p>We keep the two sample variances separate, and we do not calculate a pooled variance or sd.</p>
<hr />
<p>So, the test statistic for performing a Welch T test on the concrete beam data is
<span class="math display">\[t_{obs} \;=\; \frac{\bar{x}_{fiber} - \bar{x}_{carbon} \;-\; 0}{\sqrt{\frac{s^2_{fiber}}{n_{fiber}} + \frac{s^2_{carbon}}{n_{carbon}}}} \;=\; \frac{33.15 - 43.53 \;-\; 0}{\sqrt{\frac{2.63^2}{8} + \frac{5.06^2}{11}}} \;=\; -5.81.\]</span></p>
<p>To complete our test, we compare this observed test statistic to a t null distribution with either a rejection region or p-value. However, the null t distribution is different from the equal variances case.</p>
<p>The Welch T test is only approximate. That is to say, the test statistic <span class="math inline">\(T\)</span> has an approximate t distribution rather than an exact t distribution. The degrees of freedom in the unequal variances case is a bit messy:
<span class="math display">\[k \;=\; \frac{\Big(\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}\Big)^2}{\frac{(s^2_1/n_1)^2}{n_1-1} + \frac{(s^2_2/n_2)^2}{n_2-1}}\]</span></p>
<p>Don’t worry about understanding this entire formula. We can think of this as being a “penalty” on our degrees of freedom based on the fact that the variances being different. Different variances add additional variability, so we need to use a lower df. It can be shown that
<span class="math display">\[k \;&lt;\; n_1 + n_2 - 2\]</span>
where the left side is the unequal variances df and the right side is the equal variances df. If the sample variances <span class="math inline">\(s^2_1\)</span> and <span class="math inline">\(s^2_2\)</span> are very similar, then <span class="math inline">\(k\)</span> is only slightly less than <span class="math inline">\(n_1 + n_2 - 2\)</span>. But if <span class="math inline">\(s^2_1\)</span> and <span class="math inline">\(s^2_2\)</span> are very different, the penalty is larger and <span class="math inline">\(k\)</span> is much less than <span class="math inline">\(n_1 + n_2 - 2\)</span>.</p>
<p>The degrees of freedom <span class="math inline">\(k\)</span> will almost certainly not be a whole number. But <code>qt</code> and <code>pt</code> in R can handle decimal degrees of freedom just fine.</p>
<hr />
<p>For the concrete beam example, we have degrees of freedom <span class="math inline">\(k = 15.7\)</span>. This is less than <span class="math inline">\(n_{fiber} - n_{carbon} = 17\)</span>.</p>
<div class="infobox exer">
<p>Complete the Welch T test of hypotheses
<span class="math display">\[H_0: \mu_{fiber} - \mu_{carbon} = 0\quad\quad\text{versus}\quad\quad H_A: \mu_{fiber} - \mu_{carbon} \neq 0.\]</span>
Use test statistic <span class="math inline">\(t_{obs} = -5.81\)</span> and a null t distribution with 15.7 degrees of freedom. Use <span class="math inline">\(\alpha = 0.01\)</span>.</p>
<p><span style="color:#8601AF">
Since we are doing a two-sided test, the p-value is <span class="math inline">\(2\times\)</span> the area outside of our test statistic. Our test statistic is negative, and thus in the lower tail, so we compute a lower-tail area on the t with 15.7 degrees of freedom.
</span></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="two-sample-testing.html#cb16-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="sc">-</span><span class="fl">5.81</span>, <span class="at">df =</span> <span class="fl">15.7</span>)</span></code></pre></div>
<pre><code>## [1] 2.866725e-05</code></pre>
<p><span style="color:#8601AF">
We have a p-value of <span class="math inline">\(2.9\times 10^{-5}\)</span>. This is much smaller than 0.01, so we reject the null hypothesis. We conclude that there is a significant difference in strength between beams reinforced with fiberglass and carbon.
</span></p>
</div>
<p>The test we performed at the 1% level corresponds to a 99% confidence interval. In this context, the standard error for the CI is
<span class="math display">\[\sqrt{\frac{s^2_{fiber}}{n_{fiber}} + \frac{s^2_{carbon}}{n_{carbon}}}\]</span>
and the critical value comes from the t with 15.7 degrees of freedom. According to R, the critical value for 99% confidence is 2.928.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="two-sample-testing.html#cb18-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.995</span>, <span class="at">df =</span> <span class="fl">15.7</span>)</span></code></pre></div>
<pre><code>## [1] 2.928173</code></pre>
<p>A 99% confidence interval for <span class="math inline">\(\mu_{fiber} - \mu_{carbon}\)</span> is
<span class="math display">\[\begin{align*}
&amp; \bar{X}_{fiber} - \bar{X}_{carbon}\;\pm\; t_{15.7, 0.005} \times \sqrt{\frac{s^2_{fiber}}{n_{fiber}} + \frac{s^2_{carbon}}{n_{carbon}}} \\
&amp;= 33.15 - 43.53 \;\pm \; 2.928\times \sqrt{\frac{2.63^2}{8} + \frac{5.06^2}{11}} \\
&amp;= (-15.61, -5.15)
\end{align*}\]</span></p>
<p>This interval is entirely negative and does not cover 0.</p>
<hr />
<div class="infobox deff">
<p>In general, an <strong>unequal variances T test</strong> or <strong>Welch T test</strong> can test for any difference in means:
<span class="math display">\[H_0: \mu_1 - \mu_2 = \mu_0\quad\quad\text{versus}\quad\quad H_A: \mu_1 - \mu_2 \neq \mu_0\]</span>
(or a corresponding one-sided hypothesis). The test statistic is
<span class="math display">\[T \;=\; \frac{\bar{X}_1 - \bar{X}_2 \;-\; \mu_0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}\]</span>
and the null distribution is a t distribution. We can find a rejection region or calculate a p-value according to the direction of the hypotheses.</p>
</div>
<p>We can also complete this test in R with the <code>t.test</code> function. We specify <code>var.equal = F</code>, which is the same as the default behavior.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="two-sample-testing.html#cb20-1" tabindex="-1"></a><span class="fu">t.test</span>(fiber, carbon, <span class="at">mu =</span> <span class="dv">0</span>, <span class="at">conf.level =</span> <span class="fl">0.99</span>,</span>
<span id="cb20-2"><a href="two-sample-testing.html#cb20-2" tabindex="-1"></a>       <span class="at">var.equal =</span> F)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  fiber and carbon
## t = -5.8044, df = 15.706, p-value = 2.893e-05
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  -15.61204  -5.14251
## sample estimates:
## mean of x mean of y 
##  33.15000  43.52727</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="two-sample-testing.html#cb22-1" tabindex="-1"></a><span class="fu">t.test</span>(fiber, carbon, <span class="at">mu =</span> <span class="dv">0</span>, <span class="at">conf.level =</span> <span class="fl">0.99</span>)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  fiber and carbon
## t = -5.8044, df = 15.706, p-value = 2.893e-05
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  -15.61204  -5.14251
## sample estimates:
## mean of x mean of y 
##  33.15000  43.52727</code></pre>
<hr />
<p>The equal variances T test and Welch T test are two similar methods for testing a difference in means, <span class="math inline">\(\mu_1 - \mu_2\)</span>. How should we decide which test to use? The key differences is that the equal variances T makes the equal variance assumption. The Welch T does not make this assumption, and so it is more general.</p>
<ul>
<li><p>If the variances were truly equal, but we assumed they were not equal, the Welch T test is a bit less powerful but is still accurate.</p></li>
<li><p>If the variances are truly different, but we assumed they were equal, the equal variances test can make wildly incorrect conclusions.</p></li>
</ul>
<p>There is usually more to lose in the second case, so if there is doubt, it is safer to assume unequal variances.</p>
</div>
<div id="two-sample-proportion-test" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Two-sample proportion test<a href="two-sample-testing.html#two-sample-proportion-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous sections, we extended the concept of a T test to the two-sample case where we are testing <span class="math inline">\(\mu_1 - \mu_2\)</span>. Now, we will do the same for a proportion Z test. The two-sample proportion test can be used if we want to test a difference in two population proportions, <span class="math inline">\(\pi_1 - \pi_2\)</span>.</p>
<p>We want to know if handedness (being right vs left handed) is related to a person’s sex. To help answer this question, 21 females and 54 males were asked to indicate their dominant hand. The results were:
<span class="math display">\[\text{Female: 12 left, 9 right} \quad \quad \text{Male: 23 left, 31 right}\]</span>
We want to know if the proportion of left-handed individuals is different for males and females.</p>
<p>Let <span class="math inline">\(\pi_{FL}\)</span> be the true proportion of all females who are left handed, and <span class="math inline">\(\pi_{ML}\)</span> be the true proportion of males who are left handed. If sex and handedness are unrelated, then being left-handed would not be more or less likely based on a person’s sex. In that case, we would have <span class="math inline">\(\pi_{FL} = \pi_{ML}\)</span>. This gives hypotheses
<span class="math display">\[H_0: \pi_{FL} = \pi_{ML} \quad \text{versus} \quad H_A: \pi_{FL} \neq \pi_{ML}.\]</span>
These hypotheses can equivalently be expressed in terms of a difference in proportions:
<span class="math display">\[H_0: \pi_{FL} - \pi_{ML} = 0\quad\quad\text{versus}\quad\quad H_A: \pi_{FL} - \pi_{ML} \neq 0.\]</span></p>
<p>To answer this question, we need to look at the observed difference in left-hand proportions from our data: <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span>.</p>
<hr />
<p>We are testing whether the difference <span class="math inline">\(\pi_{FL} - \pi_{ML}\)</span> is equal to 0, so the test statistic will have the following structure.
<span class="math display">\[Z \;=\; \frac{\hat{p}_{FL} - \hat{p}_{ML} \;-\; 0 }{\text{st. error of}\; (\hat{p}_{FL} - \hat{p}_{ML})}.\]</span>
We comopare the difference to the value of 0, and divide by the standard error, which is the uncertainty from using <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span> to estimate <span class="math inline">\(\pi_{FL} - \pi_{ML}\)</span>.</p>
<p>This is a Z test based on the CLT, just like the one-sample proportion test. To get the exact formula for the test statistic, we need to consider the theoretical properties of <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span>.</p>
<hr />
<p>If the CLT assumptions are met, i.e. <span class="math inline">\(n_{FL}\)</span> and <span class="math inline">\(n_{ML}\)</span> are both large, then <span class="math inline">\(\hat{p}_{FL}\)</span> and <span class="math inline">\(\hat{p}_{ML}\)</span> are both approximately normal.
<span class="math display">\[\hat{p}_{FL} \;\dot{\sim}\; N\Big(\pi_{FL}, \; \frac{\pi_{FL}(1-\pi_{FL})}{n_{FL}}\Big), \quad \quad \hat{p}_{ML} \;\dot{\sim}\; N\Big(\pi_{ML}, \; \frac{\pi_{ML}(1-\pi_{ML})}{n_{ML}} \Big)\]</span>
The difference <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span> is also normal.
<span class="math display">\[\hat{p}_{FL} - \hat{p}_{ML} \;\;\dot{\sim}\;\; N\Big(\pi_{FL} - \pi_{ML},\; \frac{\pi_{FL}(1-\pi_{FL})}{n_{FL}} + \frac{\pi_{ML}(1-\pi_{ML})}{n_{ML}}\Big).\]</span>
The mean is the difference in true proportions. The variance has a term for both groups.</p>
<p>The above is a general statement about the difference in two proportions. Let’s think about what this looks like if we assume the null hypothesis is true and <span class="math inline">\(\pi_{FL} - \pi_{ML} = 0\)</span>. This is saying that the female and male groups both share an “overall” left-handed proportion. That is to say, <span class="math inline">\(\pi_{FL} = \pi_{ML} = \pi_L\)</span>. So if we assume <span class="math inline">\(H_0\)</span> is true, we get
<span class="math display">\[\hat{p}_{FL} - \hat{p}_{ML} \;\;\dot{\sim}\;\; N\Big(0,\; \pi_L(1-\pi_L)\Big(\frac{1}{n_{FL}} + \frac{1}{n_{ML}}\Big)\Big)\]</span></p>
<p><span class="math inline">\(\pi_L\)</span>, the true left handed proportion, is unknown, so we need to estimate it from our data. We do this by taking the total number of lefties in our sample, divided by the total number of people.
<span class="math display">\[\hat{p}_L \;=\; \frac{12 + 23}{21 + 54} \;=\; 0.467.\]</span>
So if we use <span class="math inline">\(\hat{p}_L\)</span> in place of <span class="math inline">\(\pi_L\)</span>, we get
<span class="math display">\[\hat{p}_{FL} - \hat{p}_{ML} \;\;\dot{\sim}\;\; N\Big(0,\; \hat{p}_L(1-\hat{p}_L)\Big(\frac{1}{n_{FL}} + \frac{1}{n_{ML}}\Big)\Big).\]</span></p>
<hr />
<p>The last thing we need to do to get our test statistic is to standardize the normal distribution. The normal variable is <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span>, and we are subtracting the mean of 0 and dividing by the standard deviation of <span class="math inline">\(\sqrt{\hat{p}_L(1-\hat{p}_L)\Big(\frac{1}{n_{FL}} + \frac{1}{n_{ML}}\Big)}\)</span>. If the null hypothesis is true and <span class="math inline">\(\pi_{FL} = \pi_{ML} = \pi_L\)</span>, then
<span class="math display">\[Z \;=\; \frac{\hat{p}_{FL} - \hat{p}_{ML} \;=\; 0}{\sqrt{\hat{p}_L(1-\hat{p}_L)\Big(\frac{1}{n_{FL}} + \frac{1}{n_{ML}}\Big)}} \; \dot{\sim} \; N(0, 1^2).\]</span>
The quantity <span class="math inline">\(Z\)</span> is our Z test statistic, which can be calculated from our data. We can then use a rejection region or a p-value on the standard normal curve to complete our test.</p>
<p>This is an approximate test, based on the CLT. The Z test is only accurate if both sample sizes are large enough. In general, we want the number of “successes” and the number of “failures” in each group to be at least 5.</p>
<p>As a reminder, our data is
<span class="math display">\[\text{Female: 12 left, 9 right} \quad \quad \text{Male: 23 left, 31 right}\]</span>
and we are testing hypotheses
<span class="math display">\[H_0: \pi_{FL} - \pi_{ML} = 0\quad\quad\text{versus}\quad\quad H_A: \pi_{FL} - \pi_{ML} \neq 0.\]</span></p>
<div class="infobox exer">
<p>Complete the two-sample proportion test for the handedness data, with <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<ul>
<li>Check whether the sample sizes are large enough to use the Z test.</li>
</ul>
<p><span style="color:#8601AF">
For the Z test to be accurate, we want the number of right handed and left handed individuals in each group to be at least 5. In the female group, there are 12 lefties and 9 righties. In the male group, there are 23 lefties and 31 righties. All four are greater than 5, so we can make use of the CLT.
</span></p>
<ul>
<li>Calculate the test statistic <span class="math inline">\(z_{obs}\)</span> based on an overall left-handed proportion of <span class="math inline">\(\hat{p}_L = 0.467\)</span>.</li>
</ul>
<p><span style="color:#8601AF">
The observed female left-handed proportion is <span class="math inline">\(\hat{p}_{FL} = \frac{12}{12+9} = 0.571\)</span>. The observed male left-handed proportion is <span class="math inline">\(\hat{p}_{ML} = \frac{23}{23+31} = 0.426\)</span>. So, our observed test statistic is
<span class="math display">\[z_{obs} \;=\; \frac{\hat{p}_{FL} - \hat{p}_{ML} \;=\; 0}{\sqrt{\hat{p}_L(1-\hat{p}_L)\Big(\frac{1}{n_{FL}} + \frac{1}{n_{ML}}\Big)}} \;=\; \frac{0.571 - 0.426 - 0}{\sqrt{0.467(1-0.467)\Big(\frac{1}{21}+\frac{1}{54}\Big)}} \;=\; 1.13.\]</span>
</span></p>
<ul>
<li>Compare <span class="math inline">\(z_{obs}\)</span> to a 0.05-level rejection region on the <span class="math inline">\(N(0, 1^2)\)</span> distribution, or compute a p-value.</li>
</ul>
<p><span style="color:#8601AF">
If <span class="math inline">\(H_0\)</span> is true, then the value 1.13 was drawn from a standard normal distribution. So, we need to compare this value to a standard normal curve to see if it is too extreme. Since we are doing a two-sided test, the p-value is the area outside of 1.13, multiplied by 2.
</span></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="two-sample-testing.html#cb24-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(<span class="fl">1.13</span>, <span class="at">lower.tail =</span> F)</span></code></pre></div>
<pre><code>## [1] 0.2584762</code></pre>
<p><span style="color:#8601AF">
R gives us a p-value of 0.258, which is much larger than <span class="math inline">\(\alpha = 0.05\)</span>. We fail to reject the null, and we do not have sufficient evidence that the proportion of left-handed individuals is different for males and females.
</span></p>
</div>
<hr />
<p>We can also build a corresponding Z CI for the difference <span class="math inline">\(\pi_{FL} - \pi_{ML}\)</span>. The point estimate is <span class="math inline">\(\hat{p}_{FL} - \hat{p}_{ML}\)</span>, and the critical value is <span class="math inline">\(z_{\alpha/2}\)</span> from the standard normal curve. However, we don’t use the same standard error as the hypothesis test.</p>
<p>When we are testing <span class="math inline">\(H_0: \pi_{FL} - \pi_{ML} = 0\)</span>, we calculate a common left-handed proportion <span class="math inline">\(\hat{p}_L\)</span>. But when building a CI, we aren’t assuming that null hypothesis to be true. So, we don’t use <span class="math inline">\(\hat{p}_L\)</span> and instead use the more general form for standard error.</p>
<p><span class="math display">\[\hat{se}(\hat{p}_{FL} - \hat{p}_{ML}) \;=\; \sqrt{\frac{\hat{p}_{FL}(1-\hat{p}_{FL})}{n_{FL}} + \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n_{ML}}} \;=\; \sqrt{\frac{0.571(1-0.571)}{21} + \frac{0.426(1-0.426)}{54}} \;=\; 0.127\]</span></p>
<p>The Z critical value for 95% confidence is 1.96.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="two-sample-testing.html#cb26-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<p>So, a 95% Z CI for <span class="math inline">\(\pi_{FL} - \pi_{ML}\)</span> is
<span class="math display">\[\begin{align*}
&amp; \hat{p}_{FL} - \hat{p}_{ML} \pm z_{0.025}\sqrt{\frac{\hat{p}_{FL}(1-\hat{p}_{FL})}{n_{FL}} + \frac{\hat{p}_{ML}(1-\hat{p}_{ML})}{n_{ML}}} \\
&amp;= 0.571 - 0.426 \pm 1.96\sqrt{\frac{0.571(1-0.571)}{21} + \frac{0.426(1-0.426)}{54} \\
&amp;= (-0.10, 0.39)
\end{align*}\]</span></p>
<p>The interval covers 0, which corresponds to us failing to reject <span class="math inline">\(H_0\)</span>. However, a two-sample proportion test and CI will not necessarily agree with each other, because they use slightly different formulas for standard error.</p>
<div class="infobox deff">
<p>In general, a two-sample proportion Z test can test for a difference in proportions equal to 0:
<span class="math display">\[H_0: \pi_1 - \pi_2 = 0\quad\quad\text{versus}\quad\quad H_A: \pi_1 - \pi_2 \neq 0\]</span>
(or a corresponding one-sided hypothesis). The test statistic is
<span class="math display">\[Z \;=\; \frac{\hat{p}_1 - \hat{p}_2}{\hat{p}(1-\hat{p})\Big(\frac{1}{n_1} + \frac{1}{n_2}\Big)}\]</span>
where <span class="math inline">\(\hat{p}\)</span> is the common observed proportion across both groups. The null distribution is a standard normal distribution. We find a rejection region or calculate a p-value according to the direction of the hypotheses.</p>
</div>
<div class="infobox pond">
<p>Why does the above test statistic not work for testing a difference other than 0? For example,
<span class="math display">\[H_0: \pi_1 - \pi_2 = 0.2\quad\quad\text{versus}\quad\quad H_A: \pi_1 - \pi_2 \neq 0.2\]</span></p>
</div>
</div>
<div id="two-sample-bootstrap-test" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Two-sample bootstrap test<a href="two-sample-testing.html#two-sample-bootstrap-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The last few sections in this chapter will cover different techniques for two-sample testing that do not require the data to be normal. There is the bootstrap, which is similar to the one-sample bootstrap we’ve discussed before. Lastly, we will discuss the rank sum test, which is based on the relative ranks of the data.</p>
<p>When sage crickets mate, the male allows the female to eat part of his wings. It is thought that the hunger level of the female may influence the desire to mate.</p>
<p>A group of 11 female crickets were starved for two days, and a group of 13 females were fed normally. Each female was given a male, and the time to mating (in hours) was recorded.</p>
<p>The research question is, ``Do starved females attempt mating faster than normally fed females? This can be represented as a question about the difference in average time to mate <span class="math inline">\(\mu_{started}-\mu_{fed}\)</span>, and the question also implies a one-sided alternative.
<span class="math display">\[H_0: \mu_{starved}-\mu_{fed} \ge 0\quad\quad\text{versus}\quad\quad H_A: \mu_{starved}-\mu_{fed} &lt; 0.\]</span></p>
<p>We’ve looked at two methods for testing a difference in means: the equal variances T and the Welch T. Both of these tests require independence and normality, and the equal variances T additionally requires the equal variance assumption.</p>
<hr />
<p>The cricket data is as follows.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="two-sample-testing.html#cb28-1" tabindex="-1"></a>starved <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.9</span>, <span class="fl">2.1</span>, <span class="fl">3.8</span>, <span class="fl">9.0</span>, <span class="fl">9.6</span>, <span class="fl">13.0</span>, <span class="fl">14.7</span>, <span class="fl">17.9</span>,</span>
<span id="cb28-2"><a href="two-sample-testing.html#cb28-2" tabindex="-1"></a>             <span class="fl">21.7</span>, <span class="fl">29.0</span>, <span class="fl">72.3</span>)</span>
<span id="cb28-3"><a href="two-sample-testing.html#cb28-3" tabindex="-1"></a>fed <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.5</span>, <span class="fl">1.7</span>, <span class="fl">2.4</span>, <span class="fl">3.6</span>, <span class="fl">5.7</span>, <span class="fl">22.6</span>, <span class="fl">22.8</span>, <span class="fl">39.0</span>, <span class="fl">54.4</span>,</span>
<span id="cb28-4"><a href="two-sample-testing.html#cb28-4" tabindex="-1"></a>         <span class="fl">72.1</span>, <span class="fl">73.6</span>, <span class="fl">79.5</span>, <span class="fl">88.9</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(n\)</span></th>
<th align="center"><span class="math inline">\(\bar{x}\)</span></th>
<th align="center"><span class="math inline">\(s\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Starved</td>
<td align="center">11</td>
<td align="center">17.73</td>
<td align="center">19.96</td>
</tr>
<tr class="even">
<td align="center">Fed</td>
<td align="center">13</td>
<td align="center">35.98</td>
<td align="center">33.63</td>
</tr>
</tbody>
</table>
<p>The fed group has a considerably larger center than the starved group. We also see that the fed group has a larger sample sd, so we might want to perform a Welch T test. To do this, we also need to check normality.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="two-sample-testing.html#cb29-1" tabindex="-1"></a>bin_breaks <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">18</span>, <span class="dv">36</span>, <span class="dv">54</span>, <span class="dv">72</span>, <span class="dv">90</span>)</span>
<span id="cb29-2"><a href="two-sample-testing.html#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="two-sample-testing.html#cb29-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># View 2 plots at once</span></span>
<span id="cb29-4"><a href="two-sample-testing.html#cb29-4" tabindex="-1"></a><span class="co"># Set xlim, ylim, and breaks to be the same</span></span>
<span id="cb29-5"><a href="two-sample-testing.html#cb29-5" tabindex="-1"></a><span class="fu">hist</span>(starved, <span class="at">main =</span> <span class="st">&quot;Starved&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>),</span>
<span id="cb29-6"><a href="two-sample-testing.html#cb29-6" tabindex="-1"></a>     <span class="at">breaks =</span> bin_breaks, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span>
<span id="cb29-7"><a href="two-sample-testing.html#cb29-7" tabindex="-1"></a><span class="fu">hist</span>(fed, <span class="at">main =</span> <span class="st">&quot;Fed&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>),</span>
<span id="cb29-8"><a href="two-sample-testing.html#cb29-8" tabindex="-1"></a>     <span class="at">breaks =</span> bin_breaks, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="two-sample-testing.html#cb30-1" tabindex="-1"></a><span class="fu">qqnorm</span>(starved, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Starved&quot;</span>)</span>
<span id="cb30-2"><a href="two-sample-testing.html#cb30-2" tabindex="-1"></a><span class="fu">qqnorm</span>(fed, <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Fed&quot;</span>)</span></code></pre></div>
<p><img src="09-two-sample_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
<p>The histograms and qq-plots show a strong nonnormal shape. The starved group has an outlier that gives it a strong right skew, and the fed group looks bimodal.</p>
<div class="infobox pond">
<p>If we removed the outlying value 72.3 from the starved group, the data would be more normal. Why is this not advisable? In what situations would it be appropriate to discard data?</p>
</div>
<p>Furthermore, the sample sizes are so small that we probably shouldn’t make use of the CLT. Without normality, we cannot perform a T test, because the statistic
<span class="math display">\[T \;=\; \frac{\bar{X}_1 - \bar{X}_2 \;-\; \mu_0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}\]</span>
does not have a t distribution in this case. Trying to calculate a p-value with <code>pt</code> would lead to an inaccurate result.</p>
<p>We will show how to adapt our bootstrap methods to the two-sample case, and come up with an empirical sampling distribution of <span class="math inline">\(T\)</span> that we can use to find a bootstrap p-value. The general structure of the bootstrap is the same, though the details are slightly different in the two-sample case.</p>
<hr />
<p>When we perform a bootstrap analysis, we sample <em>with replacement</em> from the data, then calculate a value <span class="math inline">\(\hat{t}\)</span> from the re-sampled data. We repeat this process and use the <span class="math inline">\(\hat{t}\)</span> values to apprxoimate the distribution of <span class="math inline">\(T\)</span>.</p>
<div class="infobox deff">
<p>The bootstrapping procedure for a two-sample test is as follows.</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(\bar{x}_1\)</span> and <span class="math inline">\(s_1^2\)</span> from the first sample, and <span class="math inline">\(\bar{x}_2\)</span> and <span class="math inline">\(s_2^2\)</span> from the second sample. Use these summaries to find the observed test statistic
<span class="math display">\[t_{obs} = \frac{\bar{x}_1 - \bar{x}_2 - \mu_0}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}.\]</span></p></li>
<li><p>Resample from <em>both</em> sets of data. Take a SRS of size <span class="math inline">\(n_1\)</span> with replacement from the first data, and a SRS of size <span class="math inline">\(n_2\)</span> with replacement from the second data.</p></li>
<li><p>Compute the means and variances of the resampled data for each group. Call these <span class="math inline">\(\bar{x}_1^*\)</span>, <span class="math inline">\(s_1^{2*}, \bar{x}_2^*\)</span>, and <span class="math inline">\(s_2^{2*}\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\hat{t}\)</span> based on the resampled data:
<span class="math display">\[\hat{t} \;=\; \frac{(\bar{x}_1^* - \bar{x}_2^*) \;-\; (\bar{x}_1 - \bar{x}_2)}{\sqrt{\frac{s_1^{2*}}{n_1}+\frac{s_2^{2*}}{n_2}}}\]</span></p></li>
<li><p>Repeat 2-4 many, many, times.</p></li>
</ol>
</div>
<p>The <span class="math inline">\(\hat{t}\)</span> values approximate the sampling distribution of <span class="math inline">\(T\)</span>. We look at the proportion of <span class="math inline">\(\hat{t}\)</span> values that are more extreme than <span class="math inline">\(t_{obs}\)</span> to calculate a p-value. Let’s perform a bootstrap test on the cricket data, with <span class="math inline">\(\alpha = 0.02\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="other-one-sample-tests.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-two-sample.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
